{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projekt ML - Michał Kowalik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline dla zbioru CIFAR-10 - regresja logistyczna w wersji multiclass\n",
    "# Przy submitowaniu predykcji proszę używać funkcji save_labels\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold\n",
    "\n",
    "import tqdm\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input, UpSampling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ładujemy dane, przy okazji przekształcając je do postaci lubianej przez keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_labels(arr, filename):\n",
    "    pd_array = pd.DataFrame(arr)\n",
    "    pd_array.index.names = [\"Id\"]\n",
    "    pd_array.columns = [\"Prediction\"]\n",
    "    pd_array.to_csv(filename)\n",
    "\n",
    "def load_labels(filename):\n",
    "    return pd.read_csv(filename, index_col=0).values.ravel()\n",
    "\n",
    "X_train = np.load(\"X_train.npy\")\n",
    "y_train = load_labels(\"y_train.csv\")\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "\n",
    "X_train_small = np.load(\"X_train_small.npy\")\n",
    "y_train_small = load_labels(\"y_train_small.csv\")\n",
    "\n",
    "y_train_one_hot = keras.utils.to_categorical(y_train)\n",
    "y_train_small_one_hot = keras.utils.to_categorical(y_train_small)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przekształcamy na float i skalujemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000L, 3072L)\n",
      "(10000L, 3072L)\n",
      "(5000L, 3072L)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "X_train_small = X_train_small.astype('float32') / 255.0\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "print X_train_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "0\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print y_train_small.max()\n",
    "print y_train_small.min()\n",
    "classes_number = y_train_small.max() - y_train_small.min() + 1\n",
    "print classes_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie dzielimy na dane trenujące i testujące. Moglibyśmy użyć cross-entropy, ale bardzo by to wydłużyło cały proces doboru parametrów. Chcemy tylko mniej-więcej wybrać parametry, a następnie i tak je będziemy testować na całym zbiorze trenującym już przy użyciu cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_s, X_te_s, y_tr_s, y_te_s = train_test_split(X_train_small, y_train_small_one_hot, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing lr: 0.01 activations: relu , softmax hidden layer size: 10\n",
      "Score:  0.2928\n",
      "Testing lr: 0.01 activations: relu , softmax hidden layer size: 50\n",
      "Score:  0.3472\n",
      "Testing lr: 0.01 activations: relu , softmax hidden layer size: 100\n",
      "Score:  0.38\n",
      "Testing lr: 0.01 activations: relu , softmax hidden layer size: 250\n",
      "Score:  0.3848\n",
      "Testing lr: 0.01 activations: relu , softmax hidden layer size: 500\n",
      "Score:  0.3848\n",
      "Testing lr: 0.01 activations: relu , softmax hidden layer size: 1000\n",
      "Score:  0.3992\n",
      "Testing lr: 0.01 activations: relu , sigmoid hidden layer size: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock C:\\Users\\Kowalik\\AppData\\Local\\Theano\\compiledir_Windows-10-10.0.15063-Intel64_Family_6_Model_69_Stepping_1_GenuineIntel-2.7.13-64\\lock_dir\\lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock C:\\Users\\Kowalik\\AppData\\Local\\Theano\\compiledir_Windows-10-10.0.15063-Intel64_Family_6_Model_69_Stepping_1_GenuineIntel-2.7.13-64\\lock_dir\\lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.2592\n",
      "Testing lr: 0.01 activations: relu , sigmoid hidden layer size: 50\n",
      "Score:  0.3216\n",
      "Testing lr: 0.01 activations: relu , sigmoid hidden layer size: 100\n",
      "Score:  0.32\n",
      "Testing lr: 0.01 activations: relu , sigmoid hidden layer size: 250\n",
      "Score:  0.348\n",
      "Testing lr: 0.01 activations: relu , sigmoid hidden layer size: 500\n",
      "Score:  0.3496\n",
      "Testing lr: 0.01 activations: relu , sigmoid hidden layer size: 1000\n",
      "Score:  0.3952\n",
      "Testing lr: 0.01 activations: softmax , softmax hidden layer size: 10\n",
      "Score:  0.216\n",
      "Testing lr: 0.01 activations: softmax , softmax hidden layer size: 50\n",
      "Score:  0.16\n",
      "Testing lr: 0.01 activations: softmax , softmax hidden layer size: 100\n",
      "Score:  0.1552\n",
      "Testing lr: 0.01 activations: softmax , softmax hidden layer size: 250\n",
      "Score:  0.0896\n",
      "Testing lr: 0.01 activations: softmax , softmax hidden layer size: 500\n",
      "Score:  0.0944\n",
      "Testing lr: 0.01 activations: softmax , softmax hidden layer size: 1000\n",
      "Score:  0.0952\n",
      "Testing lr: 0.01 activations: softmax , sigmoid hidden layer size: 10\n",
      "Score:  0.1856\n",
      "Testing lr: 0.01 activations: softmax , sigmoid hidden layer size: 50\n",
      "Score:  0.14\n",
      "Testing lr: 0.01 activations: softmax , sigmoid hidden layer size: 100\n",
      "Score:  0.1424\n",
      "Testing lr: 0.01 activations: softmax , sigmoid hidden layer size: 250\n",
      "Score:  0.1024\n",
      "Testing lr: 0.01 activations: softmax , sigmoid hidden layer size: 500\n",
      "Score:  0.0808\n",
      "Testing lr: 0.01 activations: softmax , sigmoid hidden layer size: 1000\n",
      "Score:  0.116\n",
      "Testing lr: 0.05 activations: relu , softmax hidden layer size: 10\n",
      "Score:  0.3552\n",
      "Testing lr: 0.05 activations: relu , softmax hidden layer size: 50\n",
      "Score:  0.3768\n",
      "Testing lr: 0.05 activations: relu , softmax hidden layer size: 100\n",
      "Score:  0.4248\n",
      "Testing lr: 0.05 activations: relu , softmax hidden layer size: 250\n",
      "Score:  0.4032\n",
      "Testing lr: 0.05 activations: relu , softmax hidden layer size: 500\n",
      "Score:  0.416\n",
      "Testing lr: 0.05 activations: relu , softmax hidden layer size: 1000\n",
      "Score:  0.428\n",
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 10\n",
      "Score:  0.2144\n",
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 50\n",
      "Score:  0.3736\n",
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 100\n",
      "Score:  0.3704\n",
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 250\n",
      "Score:  0.4136\n",
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 500\n",
      "Score:  0.4112\n",
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 1000\n",
      "Score:  0.4224\n",
      "Testing lr: 0.05 activations: softmax , softmax hidden layer size: 10\n",
      "Score:  0.2712\n",
      "Testing lr: 0.05 activations: softmax , softmax hidden layer size: 50\n",
      "Score:  0.256\n",
      "Testing lr: 0.05 activations: softmax , softmax hidden layer size: 100\n",
      "Score:  0.2144\n",
      "Testing lr: 0.05 activations: softmax , softmax hidden layer size: 250\n",
      "Score:  0.204\n",
      "Testing lr: 0.05 activations: softmax , softmax hidden layer size: 500\n",
      "Score:  0.1208\n",
      "Testing lr: 0.05 activations: softmax , softmax hidden layer size: 1000\n",
      "Score:  0.1008\n",
      "Testing lr: 0.05 activations: softmax , sigmoid hidden layer size: 10\n",
      "Score:  0.2136\n",
      "Testing lr: 0.05 activations: softmax , sigmoid hidden layer size: 50\n",
      "Score:  0.2328\n",
      "Testing lr: 0.05 activations: softmax , sigmoid hidden layer size: 100\n",
      "Score:  0.2008\n",
      "Testing lr: 0.05 activations: softmax , sigmoid hidden layer size: 250\n",
      "Score:  0.14\n",
      "Testing lr: 0.05 activations: softmax , sigmoid hidden layer size: 500\n",
      "Score:  0.0968\n",
      "Testing lr: 0.05 activations: softmax , sigmoid hidden layer size: 1000\n",
      "Score:  0.0864\n",
      "Testing lr: 0.1 activations: relu , softmax hidden layer size: 10\n",
      "Score:  0.3368\n",
      "Testing lr: 0.1 activations: relu , softmax hidden layer size: 50\n",
      "Score:  0.3752\n",
      "Testing lr: 0.1 activations: relu , softmax hidden layer size: 100\n",
      "Score:  0.3752\n",
      "Testing lr: 0.1 activations: relu , softmax hidden layer size: 250\n",
      "Score:  0.4\n",
      "Testing lr: 0.1 activations: relu , softmax hidden layer size: 500\n",
      "Score:  0.3944\n",
      "Testing lr: 0.1 activations: relu , softmax hidden layer size: 1000\n",
      "Score:  0.4168\n",
      "Testing lr: 0.1 activations: relu , sigmoid hidden layer size: 10\n",
      "Score:  0.2968\n",
      "Testing lr: 0.1 activations: relu , sigmoid hidden layer size: 50\n",
      "Score:  0.36\n",
      "Testing lr: 0.1 activations: relu , sigmoid hidden layer size: 100\n",
      "Score:  0.372\n",
      "Testing lr: 0.1 activations: relu , sigmoid hidden layer size: 250\n",
      "Score:  0.416\n",
      "Testing lr: 0.1 activations: relu , sigmoid hidden layer size: 500\n",
      "Score:  0.3816\n",
      "Testing lr: 0.1 activations: relu , sigmoid hidden layer size: 1000\n",
      "Score:  0.3832\n",
      "Testing lr: 0.1 activations: softmax , softmax hidden layer size: 10\n",
      "Score:  0.2624\n",
      "Testing lr: 0.1 activations: softmax , softmax hidden layer size: 50\n",
      "Score:  0.2976\n",
      "Testing lr: 0.1 activations: softmax , softmax hidden layer size: 100\n",
      "Score:  0.2152\n",
      "Testing lr: 0.1 activations: softmax , softmax hidden layer size: 250\n",
      "Score:  0.2032\n",
      "Testing lr: 0.1 activations: softmax , softmax hidden layer size: 500\n",
      "Score:  0.1856\n",
      "Testing lr: 0.1 activations: softmax , softmax hidden layer size: 1000\n",
      "Score:  0.1112\n",
      "Testing lr: 0.1 activations: softmax , sigmoid hidden layer size: 10\n",
      "Score:  0.1672\n",
      "Testing lr: 0.1 activations: softmax , sigmoid hidden layer size: 50\n",
      "Score:  0.2048\n",
      "Testing lr: 0.1 activations: softmax , sigmoid hidden layer size: 100\n",
      "Score:  0.2144\n",
      "Testing lr: 0.1 activations: softmax , sigmoid hidden layer size: 250\n",
      "Score:  0.1784\n",
      "Testing lr: 0.1 activations: softmax , sigmoid hidden layer size: 500\n",
      "Score:  0.1408\n",
      "Testing lr: 0.1 activations: softmax , sigmoid hidden layer size: 1000\n",
      "Score:  0.1064\n",
      "Testing lr: 0.2 activations: relu , softmax hidden layer size: 10\n",
      "Score:  0.3576\n",
      "Testing lr: 0.2 activations: relu , softmax hidden layer size: 50\n",
      "Score:  0.3496\n",
      "Testing lr: 0.2 activations: relu , softmax hidden layer size: 100\n",
      "Score:  0.3808\n",
      "Testing lr: 0.2 activations: relu , softmax hidden layer size: 250\n",
      "Score:  0.3456\n",
      "Testing lr: 0.2 activations: relu , softmax hidden layer size: 500\n",
      "Score:  0.3416\n",
      "Testing lr: 0.2 activations: relu , softmax hidden layer size: 1000\n",
      "Score:  0.3888\n",
      "Testing lr: 0.2 activations: relu , sigmoid hidden layer size: 10\n",
      "Score:  0.3032\n",
      "Testing lr: 0.2 activations: relu , sigmoid hidden layer size: 50\n",
      "Score:  0.364\n",
      "Testing lr: 0.2 activations: relu , sigmoid hidden layer size: 100\n",
      "Score:  0.3944\n",
      "Testing lr: 0.2 activations: relu , sigmoid hidden layer size: 250\n",
      "Score:  0.3568\n",
      "Testing lr: 0.2 activations: relu , sigmoid hidden layer size: 500\n",
      "Score:  0.3712\n",
      "Testing lr: 0.2 activations: relu , sigmoid hidden layer size: 1000\n",
      "Score:  0.3384\n",
      "Testing lr: 0.2 activations: softmax , softmax hidden layer size: 10\n",
      "Score:  0.316\n",
      "Testing lr: 0.2 activations: softmax , softmax hidden layer size: 50\n",
      "Score:  0.2904\n",
      "Testing lr: 0.2 activations: softmax , softmax hidden layer size: 100\n",
      "Score:  0.2376\n",
      "Testing lr: 0.2 activations: softmax , softmax hidden layer size: 250\n",
      "Score:  0.2528\n",
      "Testing lr: 0.2 activations: softmax , softmax hidden layer size: 500\n",
      "Score:  0.252\n",
      "Testing lr: 0.2 activations: softmax , softmax hidden layer size: 1000\n",
      "Score:  0.2072\n",
      "Testing lr: 0.2 activations: softmax , sigmoid hidden layer size: 10\n",
      "Score:  0.2808\n",
      "Testing lr: 0.2 activations: softmax , sigmoid hidden layer size: 50\n",
      "Score:  0.2392\n",
      "Testing lr: 0.2 activations: softmax , sigmoid hidden layer size: 100\n",
      "Score:  0.2672\n",
      "Testing lr: 0.2 activations: softmax , sigmoid hidden layer size: 250\n",
      "Score:  0.1992\n",
      "Testing lr: 0.2 activations: softmax , sigmoid hidden layer size: 500\n",
      "Score:  0.172\n",
      "Testing lr: 0.2 activations: softmax , sigmoid hidden layer size: 1000\n",
      "Score:  0.1376\n"
     ]
    }
   ],
   "source": [
    "# Najpierw zobaczymy jak sobie radzi na prostej sieci 3-warstwowej na malych danych\n",
    "\n",
    "for lr in [0.01, 0.05, 0.1, 0.2]:\n",
    "    for activation_first in ['relu', 'softmax']:\n",
    "        for activation_second in ['softmax', 'sigmoid']:\n",
    "            for hidden_size in [10, 50, 100, 250, 500, 1000]:\n",
    "\n",
    "                batch = 1000\n",
    "\n",
    "                model = Sequential()\n",
    "                model.add(Dense(hidden_size, input_shape=(X_train_small.shape[1],)))\n",
    "                model.add(Activation(activation_first))\n",
    "                model.add(Dense(classes_number, input_shape=(hidden_size, )))\n",
    "                model.add(Activation(activation_second))\n",
    "\n",
    "                model.compile(optimizer=SGD(lr=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "                print \"Testing lr:\", lr, \"activations:\", activation_first, \",\", activation_second, \\\n",
    "                    \"hidden layer size:\", hidden_size\n",
    "\n",
    "                model.fit(X_tr_s, y_tr_s, epochs=50, batch_size=batch, verbose=0)\n",
    "\n",
    "                y_pred = model.predict(X_te_s, batch_size=batch)\n",
    "\n",
    "                print \"Score: \", accuracy_score(y_te_s.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, ze siec osiaga najlepsze wyniki dla lr=0.05, funkcji aktywacji relu, sigmoid i wiekosci warsty ukrytej na poziomie 500-1000. Zatem sprobujmy wytrenowac siec na dla calych danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 500\n",
      "Running Fold 1 / 3\n",
      "Score:  0.50575884823\n",
      "Running Fold 2 / 3\n",
      "Score:  0.456328734253\n",
      "Running Fold 3 / 3\n",
      "Score:  0.467647058824\n",
      "Score mean:  0.476578213769\n",
      "\n",
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 1000\n",
      "Running Fold 1 / 3\n",
      "Score:  0.472765446911\n",
      "Running Fold 2 / 3\n",
      "Score:  0.453929214157\n",
      "Running Fold 3 / 3\n",
      "Score:  0.476770708283\n",
      "Score mean:  0.467821789784\n",
      "\n",
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 1500\n",
      "Running Fold 1 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock C:\\Users\\Kowalik\\AppData\\Local\\Theano\\compiledir_Windows-10-10.0.15063-Intel64_Family_6_Model_69_Stepping_1_GenuineIntel-2.7.13-64\\lock_dir\\lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock C:\\Users\\Kowalik\\AppData\\Local\\Theano\\compiledir_Windows-10-10.0.15063-Intel64_Family_6_Model_69_Stepping_1_GenuineIntel-2.7.13-64\\lock_dir\\lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.511937612478\n",
      "Running Fold 2 / 3\n",
      "Score:  0.495440911818\n",
      "Running Fold 3 / 3\n",
      "Score:  0.502040816327\n",
      "Score mean:  0.503139780207\n",
      "\n",
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 2000\n",
      "Running Fold 1 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock C:\\Users\\Kowalik\\AppData\\Local\\Theano\\compiledir_Windows-10-10.0.15063-Intel64_Family_6_Model_69_Stepping_1_GenuineIntel-2.7.13-64\\lock_dir\\lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock C:\\Users\\Kowalik\\AppData\\Local\\Theano\\compiledir_Windows-10-10.0.15063-Intel64_Family_6_Model_69_Stepping_1_GenuineIntel-2.7.13-64\\lock_dir\\lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.490161967606\n",
      "Running Fold 2 / 3\n",
      "Score:  0.478764247151\n",
      "Running Fold 3 / 3\n",
      "Score:  0.506842737095\n",
      "Score mean:  0.491922983951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.05\n",
    "activation_first = 'relu'\n",
    "activation_second = 'sigmoid'\n",
    "hidden_sizes = [500, 1000, 1500, 2000]\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    \n",
    "    n_folds = 3\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    \n",
    "    print \"Testing lr:\", lr, \"activations:\", activation_first, \",\", activation_second, \\\n",
    "        \"hidden layer size:\", hidden_size\n",
    "        \n",
    "    scores = []\n",
    "\n",
    "    for i, (train, test) in enumerate(skf.split(X_tr, y_train)):\n",
    "        print \"Running Fold\", i+1, \"/\", n_folds\n",
    "\n",
    "        batch = 1000\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hidden_size, input_shape=(X_tr.shape[1],)))\n",
    "        model.add(Activation(activation_first))\n",
    "        model.add(Dense(classes_number, input_shape=(hidden_size, )))\n",
    "        model.add(Activation(activation_second))\n",
    "\n",
    "        model.compile(optimizer=SGD(lr=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "        model.fit(X_tr[train], y_train_one_hot[train], epochs=50, batch_size=batch, verbose=0)\n",
    "        \n",
    "        y_pred = model.predict(X_tr[test], batch_size=batch)\n",
    "        \n",
    "        score = accuracy_score(y_train_one_hot[test].argmax(axis=1), y_pred.argmax(axis=1))\n",
    "        print \"Score: \", score\n",
    "        scores.append(score)\n",
    "\n",
    "    print \"Score mean: \", np.mean(scores)\n",
    "    print \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy zatem, że na prostej sieci newuronowej z jedną warstwą ukrytą, na cross-entropy estymator z 3-ma foldami na zbiorze Train osiąga wynik na poziomie 50% accuracy, przy learning rate 0.05, funkcji aktywacji pierwszej warstwy: relu i funkcji aktywacji drugiej warstwy: sigmoid i rozmiarze ukrytej warstwy na poziomie 1500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moglibyśmy jeszcze dobrać optymalną liczbę epok uczenia, jednak zapewne wynik uda nam się podbić nieznacznie, zatem spróbujemy użyć innej metody, jaką jest prosta sieć konwolucyjna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_cifar, y_train_cifar), (X_test_cifar, y_test_cifar) = cifar10.load_data()\n",
    "\n",
    "X_train_cifar = X_train_cifar.astype('float32')\n",
    "X_test_cifar = X_test_cifar.astype('float32')\n",
    "X_train_cifar = X_train_cifar / 255.0\n",
    "X_test_cifar = X_test_cifar / 255.0\n",
    "\n",
    "y_train_cifar = np_utils.to_categorical(y_train_cifar)\n",
    "y_test_cifar = np_utils.to_categorical(y_test_cifar)\n",
    "num_classes = y_test_cifar.shape[1]\n",
    "                                 \n",
    "print X_train_cifar\n",
    "\n",
    "print \"BLABLABLABLABLABLABLABLABLABLABLABLABLABLABLABLABLABLABLABLABLA\"\n",
    "print pack_color_image(X_train).reshape(-1, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pack_color_image(tab):\n",
    "    tab_red = tab[:,:1024]\n",
    "    tab_green = tab[:,1024:2048]\n",
    "    tab_blue = tab[:,2048:3072]\n",
    "\n",
    "    ret = np.dstack((tab_red, tab_green, tab_blue))\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750L, 32L, 32L, 3L)\n",
      "(1250L, 32L, 32L, 3L)\n",
      "(3750L, 10L)\n",
      "(1250L, 10L)\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "X_tr_s_reshaped = pack_color_image(X_tr_s).reshape(-1, 32, 32, 3)\n",
    "X_te_s_reshaped = pack_color_image(X_te_s).reshape(-1, 32, 32, 3)\n",
    "print X_tr_s_reshaped.shape\n",
    "print X_te_s_reshaped.shape\n",
    "\n",
    "print y_tr_s.shape\n",
    "print y_te_s.shape\n",
    "\n",
    "num_classes = y_te_s.shape[1]\n",
    "print num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock C:\\Users\\Kowalik\\AppData\\Local\\Theano\\compiledir_Windows-10-10.0.15063-Intel64_Family_6_Model_69_Stepping_1_GenuineIntel-2.7.13-64\\lock_dir\\lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock C:\\Users\\Kowalik\\AppData\\Local\\Theano\\compiledir_Windows-10-10.0.15063-Intel64_Family_6_Model_69_Stepping_1_GenuineIntel-2.7.13-64\\lock_dir\\lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_74 (Conv2D)           (None, 32, 32, 32)        2432      \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_75 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_346 (Dense)            (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_347 (Dense)            (None, 10L)               5130      \n",
      "=================================================================\n",
      "Total params: 4,211,626\n",
      "Trainable params: 4,211,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(32, 32, 3), padding='same', activation='relu', kernel_constraint=maxnorm(3)))#, kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))#, kernel_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))#, kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "epochs = 10\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3750 samples, validate on 1250 samples\n",
      "Epoch 1/10\n",
      "3750/3750 [==============================] - ETA: 24s - loss: 2.3001 - acc: 0.06 - ETA: 24s - loss: 2.3395 - acc: 0.03 - ETA: 23s - loss: 2.3378 - acc: 0.08 - ETA: 23s - loss: 2.3572 - acc: 0.08 - ETA: 23s - loss: 2.3442 - acc: 0.10 - ETA: 23s - loss: 2.3314 - acc: 0.11 - ETA: 22s - loss: 2.3201 - acc: 0.11 - ETA: 22s - loss: 2.3209 - acc: 0.11 - ETA: 22s - loss: 2.3181 - acc: 0.12 - ETA: 22s - loss: 2.3121 - acc: 0.12 - ETA: 21s - loss: 2.3171 - acc: 0.12 - ETA: 21s - loss: 2.3131 - acc: 0.12 - ETA: 21s - loss: 2.3118 - acc: 0.12 - ETA: 21s - loss: 2.3102 - acc: 0.13 - ETA: 21s - loss: 2.3088 - acc: 0.13 - ETA: 21s - loss: 2.3121 - acc: 0.12 - ETA: 20s - loss: 2.3069 - acc: 0.13 - ETA: 20s - loss: 2.3071 - acc: 0.12 - ETA: 20s - loss: 2.3052 - acc: 0.13 - ETA: 20s - loss: 2.3041 - acc: 0.13 - ETA: 19s - loss: 2.3035 - acc: 0.13 - ETA: 19s - loss: 2.3023 - acc: 0.13 - ETA: 19s - loss: 2.3012 - acc: 0.13 - ETA: 19s - loss: 2.2974 - acc: 0.14 - ETA: 19s - loss: 2.2922 - acc: 0.14 - ETA: 18s - loss: 2.2917 - acc: 0.14 - ETA: 18s - loss: 2.2929 - acc: 0.14 - ETA: 18s - loss: 2.2925 - acc: 0.14 - ETA: 18s - loss: 2.2922 - acc: 0.14 - ETA: 18s - loss: 2.2896 - acc: 0.14 - ETA: 17s - loss: 2.2879 - acc: 0.14 - ETA: 17s - loss: 2.2865 - acc: 0.14 - ETA: 17s - loss: 2.2874 - acc: 0.13 - ETA: 17s - loss: 2.2836 - acc: 0.13 - ETA: 17s - loss: 2.2828 - acc: 0.13 - ETA: 16s - loss: 2.2825 - acc: 0.13 - ETA: 16s - loss: 2.2800 - acc: 0.13 - ETA: 16s - loss: 2.2799 - acc: 0.13 - ETA: 16s - loss: 2.2758 - acc: 0.14 - ETA: 16s - loss: 2.2741 - acc: 0.14 - ETA: 15s - loss: 2.2741 - acc: 0.14 - ETA: 15s - loss: 2.2738 - acc: 0.14 - ETA: 15s - loss: 2.2739 - acc: 0.14 - ETA: 15s - loss: 2.2736 - acc: 0.14 - ETA: 15s - loss: 2.2726 - acc: 0.14 - ETA: 14s - loss: 2.2733 - acc: 0.14 - ETA: 14s - loss: 2.2726 - acc: 0.14 - ETA: 14s - loss: 2.2700 - acc: 0.14 - ETA: 14s - loss: 2.2688 - acc: 0.14 - ETA: 13s - loss: 2.2711 - acc: 0.14 - ETA: 13s - loss: 2.2727 - acc: 0.14 - ETA: 13s - loss: 2.2711 - acc: 0.14 - ETA: 13s - loss: 2.2705 - acc: 0.14 - ETA: 13s - loss: 2.2659 - acc: 0.14 - ETA: 12s - loss: 2.2646 - acc: 0.14 - ETA: 12s - loss: 2.2668 - acc: 0.14 - ETA: 12s - loss: 2.2664 - acc: 0.14 - ETA: 12s - loss: 2.2661 - acc: 0.14 - ETA: 12s - loss: 2.2651 - acc: 0.14 - ETA: 11s - loss: 2.2644 - acc: 0.15 - ETA: 11s - loss: 2.2640 - acc: 0.15 - ETA: 11s - loss: 2.2642 - acc: 0.14 - ETA: 11s - loss: 2.2647 - acc: 0.14 - ETA: 11s - loss: 2.2648 - acc: 0.14 - ETA: 10s - loss: 2.2629 - acc: 0.15 - ETA: 10s - loss: 2.2610 - acc: 0.15 - ETA: 10s - loss: 2.2566 - acc: 0.15 - ETA: 10s - loss: 2.2591 - acc: 0.15 - ETA: 10s - loss: 2.2566 - acc: 0.15 - ETA: 9s - loss: 2.2565 - acc: 0.1540 - ETA: 9s - loss: 2.2566 - acc: 0.152 - ETA: 9s - loss: 2.2557 - acc: 0.153 - ETA: 9s - loss: 2.2577 - acc: 0.152 - ETA: 8s - loss: 2.2573 - acc: 0.152 - ETA: 8s - loss: 2.2568 - acc: 0.151 - ETA: 8s - loss: 2.2556 - acc: 0.153 - ETA: 8s - loss: 2.2539 - acc: 0.153 - ETA: 8s - loss: 2.2520 - acc: 0.153 - ETA: 7s - loss: 2.2503 - acc: 0.155 - ETA: 7s - loss: 2.2498 - acc: 0.156 - ETA: 7s - loss: 2.2499 - acc: 0.156 - ETA: 7s - loss: 2.2480 - acc: 0.158 - ETA: 7s - loss: 2.2456 - acc: 0.159 - ETA: 6s - loss: 2.2449 - acc: 0.160 - ETA: 6s - loss: 2.2416 - acc: 0.161 - ETA: 6s - loss: 2.2391 - acc: 0.162 - ETA: 6s - loss: 2.2373 - acc: 0.162 - ETA: 6s - loss: 2.2362 - acc: 0.162 - ETA: 5s - loss: 2.2337 - acc: 0.162 - ETA: 5s - loss: 2.2318 - acc: 0.163 - ETA: 5s - loss: 2.2307 - acc: 0.163 - ETA: 5s - loss: 2.2291 - acc: 0.163 - ETA: 5s - loss: 2.2273 - acc: 0.164 - ETA: 4s - loss: 2.2261 - acc: 0.165 - ETA: 4s - loss: 2.2252 - acc: 0.165 - ETA: 4s - loss: 2.2219 - acc: 0.167 - ETA: 4s - loss: 2.2222 - acc: 0.167 - ETA: 3s - loss: 2.2203 - acc: 0.169 - ETA: 3s - loss: 2.2208 - acc: 0.169 - ETA: 3s - loss: 2.2189 - acc: 0.170 - ETA: 3s - loss: 2.2180 - acc: 0.170 - ETA: 3s - loss: 2.2154 - acc: 0.172 - ETA: 2s - loss: 2.2144 - acc: 0.172 - ETA: 2s - loss: 2.2137 - acc: 0.172 - ETA: 2s - loss: 2.2120 - acc: 0.173 - ETA: 2s - loss: 2.2111 - acc: 0.173 - ETA: 2s - loss: 2.2102 - acc: 0.174 - ETA: 1s - loss: 2.2099 - acc: 0.174 - ETA: 1s - loss: 2.2069 - acc: 0.176 - ETA: 1s - loss: 2.2052 - acc: 0.177 - ETA: 1s - loss: 2.2046 - acc: 0.177 - ETA: 1s - loss: 2.2038 - acc: 0.177 - ETA: 0s - loss: 2.2033 - acc: 0.177 - ETA: 0s - loss: 2.2013 - acc: 0.178 - ETA: 0s - loss: 2.2001 - acc: 0.178 - ETA: 0s - loss: 2.1993 - acc: 0.178 - ETA: 0s - loss: 2.1984 - acc: 0.179 - 27s - loss: 2.1977 - acc: 0.1792 - val_loss: 2.1174 - val_acc: 0.2224\n",
      "Epoch 2/10\n",
      " 416/3750 [==>...........................] - ETA: 25s - loss: 2.1769 - acc: 0.12 - ETA: 25s - loss: 2.2460 - acc: 0.14 - ETA: 24s - loss: 2.2041 - acc: 0.17 - ETA: 24s - loss: 2.1866 - acc: 0.17 - ETA: 24s - loss: 2.2125 - acc: 0.17 - ETA: 23s - loss: 2.1861 - acc: 0.18 - ETA: 23s - loss: 2.1547 - acc: 0.21 - ETA: 23s - loss: 2.1531 - acc: 0.21 - ETA: 22s - loss: 2.1534 - acc: 0.21 - ETA: 22s - loss: 2.1491 - acc: 0.20 - ETA: 22s - loss: 2.1378 - acc: 0.21 - ETA: 22s - loss: 2.1366 - acc: 0.20 - ETA: 22s - loss: 2.1191 - acc: 0.2139"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-258-8b45c244adf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr_s_reshaped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tr_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_te_s_reshaped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_te_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\keras\\models.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    868\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1511\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1158\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1160\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1161\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\keras\\backend\\theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1191\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1193\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\theano\\compile\\function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda2\\lib\\site-packages\\theano\\gof\\op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    891\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_tr_s_reshaped, y_tr_s, validation_data=(X_te_s_reshaped, y_te_s), epochs=epochs, batch_size=32)\n",
    "scores = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49016196760647868, 0.47876424715056987, 0.50684273709483796]\n"
     ]
    }
   ],
   "source": [
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_te_s_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.5088\n"
     ]
    }
   ],
   "source": [
    "score = accuracy_score(y_te_s.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print \"Score: \", score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spróbujmy przetestować jeszcze jeden model, którym będzie RandomForest. Najpierw, tak samo, zrobimy evaluację na małych danych, a później nauczymy model na pełnych danych trenujących."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_estimators\n",
    "\n",
    "rf = R"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
