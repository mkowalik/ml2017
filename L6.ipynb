{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L6\n",
    "\n",
    "Na tych ćwiczeniach omówimy:\n",
    "* model evaluation,\n",
    "* model selection,\n",
    "* data augmentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation - przypomnienie\n",
    "\n",
    "Najprostszym sposobem na ocenę skuteczności modelu jest podzielenie danych w losowy sposób na zbiory trenujący i testowy, wytrenowanie modelu na zbiorze trenującym i przetestowanie na zbiorze testowym. Cross validation stosujemy w celu zmniejszenia wariancji estymatora losowego podziału.\n",
    "\n",
    "### Zasada działania\n",
    "\n",
    "Robimy jeden duży podział - dzielimy cały zbiór na $k$ rozłącznych podzbiorów i potem $k$ razy zbiór trenujący składamy z $k-1$ kawałków, a ostatniego używamy jako zbiór testowy. Dzięki temu każdy element $D$ będzie dokładnie raz w zbiorze testowym.\n",
    "\n",
    "<img src=\"figures/L3/K-fold_cross_validation_EN.jpg\">\n",
    "\n",
    "### Leave one out\n",
    "\n",
    "Szczególnym przypadkiem jest podejście leave one out, gdzie zbiór testowy jest zawsze jednoelementowy ($k$ jest równe liczbie przykładów)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "Model evaluation to cała rodzina algorytmów estymowania skuteczności nauczonego modelu. Oczywiście interesuje nas to, jak model poradzi sobie na danych, które będą pojawiały się \"w prawdziwym świecie\", a więc na danych pochodzących z tego samego źródła, co zbiór treningowy, lecz niekoniecznie identycznych (dane takie same, a nie te same).\n",
    "\n",
    "### Dane i.i.d.\n",
    "\n",
    "W najprostszym przypadku zakładamy, że dane pochodzą z tego samego rozkładu, więc możemy np.:\n",
    "* podzielić dane losowo na zbiór trenujący i testowy,\n",
    "* wykonać powyższe wielokrotnie i uśrednić wynik,\n",
    "* użyć cross validation,\n",
    "* użyć leave one out.\n",
    "\n",
    "Za każdym razem model jest uczony od nowa na zbiorze treningowym, a testowany tylko na danych ze zbioru testowego.\n",
    "\n",
    "W wypadku problemów klasyfikacji podział train/test możemy wykonać w sposób \"stratified\" - jeśli np. zbiór testowy stanowi 30% wszystkich danych, to dodatkowo wymuszamy, aby 30% danych z **każdej klasy** znalazło się w zbiorze testowym. Jest to szczególnie ważne w wypadku, gdy niektóre klasy są mało liczebne w naszych danych - w ten sposób zapobiegamy sytuacji, w której cała klasa zostanie pominięta w zbiorze treningowym lub testowym.\n",
    "\n",
    "### Dane nie i.i.d.\n",
    "\n",
    "Oczywiście w praktyce nie zawsze można sensownie uargumentować i.i.d. Przykładem niech będzie zbiór MNIST - każdy człowiek ma nieco inny charakter pisma, a zadanie polega na nauczeniu modelu, który dobrze radzi sobie na danych pochodzących od nowych ludzi. Dlatego też twórca MNISTa dostarcza podział train/test, który wykonany jest względem ludzi (w zbiorze treningowym znajdują się cyfry napisane przez ok. 250 osób, łącznie 60 tys. przykładów, w zbiorze testowym znajduje się 10 tys. przykładów pochodzących od innych osób). Dzięki temu sprawdzamy, czy model faktycznie generalizuje na nowy charakter pisma, a nie tylko zapamiętuje poszczególne osoby i ich styl.\n",
    "\n",
    "W tym notebooku będziemy dla prostoty używali wszystkich danych łącznie, ale należy pamiętać, co w ten sposób tak naprawdę sprawdzamy.\n",
    "\n",
    "### Szeregi czasowe\n",
    "\n",
    "Bardzo często dane pochodzą z szeregu czasowego, tzn. wiersze są postaci ($\\mathbf{x}$, $y$, timestamp). Jeśli zadanie polega na generalizacji w czasie - czyli oczekujemy, że nasz model będzie działał dobrze w przyszłości - to przy ewaluacji musimy wykonać podział train/test ze względu na czas (train to wszystkie dane przed pewną ustaloną datą, test to dane późniejsze). Temat szeregów czasowych wykracza poza ramy tego notebooka. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "Modele opisane są dwoma zestawami liczb: parametrami i hiperparametrami.\n",
    "\n",
    "### Parametry\n",
    "\n",
    "Parametry to zmienne, które wyznaczane są algorytmicznie podczas uczenia modelu (jawnym wzorem, metodami iteracyjnymi itp.). Przykładem są wagi w regresji liniowej, wagi w sieciach neuronowych, support vectors w SVMach, progi w drzewach decyzyjnych.\n",
    "\n",
    "### Hiperparametry\n",
    "\n",
    "W odróżnieniu od parametrów, hiperparametry muszą być dobrane ręcznie przed rozpoczęciem uczenia. Hiperparametry kontrolują działanie modelu, pozwalają manipulować overfittingiem itp. Przykłady: stała C w regresji liniowej, architektura sieci neuronowej (liczba i rozmiar warstw, funkcje aktywacji), wymiar podprzestrzeni w PCA, liczba drzew w Random Forest.\n",
    "\n",
    "### Motywacja\n",
    "\n",
    "W założeniu hiperparametry pozwalają włączyć człowieka w proces uczenia modelu - człowiek potrafi dobrać hiperparametry, ponieważ posiada intuicję dotyczącą prawdziwego świata i, w przeciwieństwie do maszyny, rozumie dany task znacznie głębiej oraz potrafi poprawnie zinterpretować działanie hiperparametrów. Natomiast znalezieniem parametrów, co jest zadaniem żmudnym, ale dobrze zdefiniowanym, może zająć się komputer.\n",
    "\n",
    "Oczywiście to wszystko nieprawda - człowiek wcale nie ma lepszej intuicji, a zezwolenie na ręczny dobór hiperparametrów prowadzi do różnych patologii (np. testujemy dziesiątki tysięcy zestawów hiperparametów i publikujemy najlepszy wynik - zamiast dostać lepszy model zoverfitowaliśmy się do danych).\n",
    "\n",
    "Dlatego przyjmijmy na tę chwilę inną definicję - hiperparametry (tym razem z definicji!) różnią się od parametrów tym, że nie da się ich zoptymalizować gradientowo lub jawnym wzorem. Model selection to po prostu algorytm doboru hiperparametrów.\n",
    "\n",
    "### Przykład\n",
    "\n",
    "Załóżmy, że mamy zbiór (X_train, y_train) oraz klasę Model, która przyjmuje w konstruktorze zestaw hiperparametrów. Możemy zrobić następująco:\n",
    "* stworzyć listę zestawów hiperparametrów i dla każdego stworzyć obiekt model = Model(zestaw_hiperparametrów),\n",
    "* ocenić wszystkie modele na danych (X_train, y_train),\n",
    "* wybrać ten zestaw hiperparametrów, dla którego model uzyskał najlepszy wynik.\n",
    "\n",
    "### Jak stworzyć listę zestawów hiperparametrów\n",
    "\n",
    "Zamieniliśmy problem ręcznego doboru hiperparametrów na ręczne tworzenie listy zestawów hiperparametrów - czy w takim razie wciąż nie oszukujemy? Trochę tak, ale na szczęście trochę mniej.\n",
    "\n",
    "Jak w praktyce tworzy się listę zestawów hiperparametrów:\n",
    "1. Grid search - dla każdego hiperparametru arbitralnie tworzymy listę jego możliwych wartości, a następnie sprawdzamy wszystkie możliwe zestawy (problem: dla 10 hiperparametrów i 10 możliwości każdego z nich musimy wytrenować $10^{10}$ modelów).\n",
    "2. Random grid search - robimy jak powyżej, ale losujemy $n$ zestawów ($n$ dobieramy ręcznie, np. 43) zamiast sprawdzać wszystkie.\n",
    "3. Random search - dla danego hiperparametru nie definiujemy listy jego możliwych wartości, ale rozkład, z którego będziemy go losowali (np. losujemy $\\alpha$ z rozkładu jednostajnego na odcinku $[-5,5]$, a potem definiujemy hiperparametr regresji liniowej $C=10^{\\alpha}$); następnie zestaw hiperparametrów tworzymy losując każdy niezależnie z podanego rozkładu.\n",
    "\n",
    "Podejście trzecie jest zdecydowanie najlepsze, co obrazuje poniższa ilustracja [proszę spróbować wytłumaczyć].\n",
    "\n",
    "<img width=600 src=\"figures/L6/scikitlearn8.jpeg\">\n",
    "\n",
    "### * Optymalizacja bayesowska\n",
    "\n",
    "Można robić jeszcze lepiej, niż random search. Rozkłady podane w random search traktujemy jako naszą wiedzę a priori o położeniu najlepszego zestawu hiperparametrów, a następnie po każdym losowaniu ewaluujemy model i uzyskany wynik włączamy do naszej wiedzy. Szczegóły są nieco skomplikowane (musimy założyć trochę dodatkowych własności o tym, jak wynik modelu zależy od położenia w przestrzeni hiperparametrów), natomiast zainteresowane osoby mogą spróbować zapoznać się np. z następującymi implementacjami:\n",
    "* http://hyperopt.github.io/hyperopt/\n",
    "* https://github.com/msmbuilder/osprey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podsumowanie\n",
    "\n",
    "Zakładamy, że Model to klasa, a model to instancja. Model przyjmuje w konstruktorze wszystkie hiperparametry. Dane to (X,y). Model selection i model evaluation wykonujemy przy pomocy 3-fold cross validation. Ustalamy też listę zestawów hiperparametrów.\n",
    "\n",
    "### Ważna i subtelna uwaga\n",
    "\n",
    "Techniki model evaluation możemy stosować tylko do konkretnych algorytmów. Dlatego nie możemy zewaluować regresji liniowiej, tylko np. regresję liniową ze stałą C=3. Nie możemy ewaluować klasy Model, tylko obiekt model.\n",
    "\n",
    "Ale gdy zdecydujemy się na pewien algorytm doboru hiperparametrów, to możemy zapisać:\n",
    "\n",
    "(Model + model_selection) = metamodel\n",
    "\n",
    "gdzie metamodel da się fitować (parametry **oraz hiperparametry** zostaną dobrane automatycznie tylko na podstawie danych). W tym sensie możemy ewaluować klasę Model.\n",
    "\n",
    "Warto też zwrócić uwagę, że dla różnych model_selection dostaniemy różne metamodele. Np. jeśli raz za model_selection użyjemy strategii cross validation z 3 foldami, a za drugim razem np. leave one out, to te dwa metamodele będą miały różną zasadę działania i przez to inny score (nawet jeśli w obu przypadkach Model to np. regresja liniowa).\n",
    "\n",
    "### Algorytmy\n",
    "\n",
    "Aby zewaluować Model (metamodel):\n",
    "1. Dzielimy trzy razy (X,y) na (X_train,y_train), (X_test,y_test).\n",
    "2. Dla każdego takiego podziału:\n",
    "    1. Dzielimy trzy razy (X_train,y_train) na (X_train2,y_train2), (X_valid,y_valid).\n",
    "    2. Dla każdego zestawu hiperparametrów:\n",
    "        1. Dla każdego podziału (X_train2,y_train2), (X_valid,y_valid):\n",
    "            1. Tworzymy model = Model(zestaw_hiperparametrów).\n",
    "            2. Uczymy model na (X_train2,y_train2), testujemy na (X_valid,y_valid) i otrzymujemy score.\n",
    "            3. (zestaw_hiperparametrów, score) zapisujemy w tabelce.\n",
    "    3. Dla każdego zestawu hiperparametrów mamy trzy różne score z trzech podziałów - uśredniamy je.\n",
    "    4. Wybieramy średnio najlepszy zestaw hiperparametrów.\n",
    "    5. Tworzymy model = Model(średnio_najlepszy_zestaw_hiperparametrów) i uczymy na całym (X_train,y_train).\n",
    "    6. Testujemy model na (X_test,y_test) i otrzymujemy evaluation_score, zapisujemy go.\n",
    "3. Uśredniamy trzy evaluation_score, zwracamy średnią jako ostateczny score Modelu.\n",
    "\n",
    "Uwaga - dla różnych (X_train,y_train) mamy różne ostatecznie wybrane średnio_najlepszy_zestaw_hiperparametrów, tak właśnie ma być [proszę się zastanowić, dlaczego].\n",
    "\n",
    "Aby wytrenować metamodel:\n",
    "1. Dzielimy trzy razy (X,y) na (X_train,y_train), (X_valid,y_valid).\n",
    "2. Dla każdego zestawu hiperparametrów:\n",
    "    1. Dla każdego podziału (X_train,y_train), (X_valid,y_valid):\n",
    "        1. Tworzymy model = Model(zestaw_hiperparametrów).\n",
    "        2. Uczymy model na (X_train,y_train), testujemy na (X_valid,y_valid) i otrzymujemy score.\n",
    "        3. (zestaw_hiperparametrów, score) zapisujemy w tabelce.\n",
    "3. Dla każdego zestawu hiperparametrów mamy trzy różne score z trzech podziałów - uśredniamy je.\n",
    "4. Wybieramy średnio najlepszy zestaw hiperparametrów.\n",
    "5. Tworzymy model = Model(średnio_najlepszy_zestaw_hiperparametrów) i uczymy na całym (X,y).\n",
    "\n",
    "Uwaga - teraz mamy tylko jeden średnio_najlepszy_zestaw_hiperparametrów i jeden model.\n",
    "\n",
    "W ten sposób otrzymujemy:\n",
    "1. model - ten model (wytrenowany metamodel) idzie do produkcji.\n",
    "2. score - estymacja skuteczności klasy Model jest dość dobrą estymacją tego, jak model poradzi sobie w przyszłości.\n",
    "3. średnio_najlepszy_zestaw_hiperparametrów - informacyjnie możemy też podać znaleziony najlepszy zestaw hiperparametrów, jednakże w tym momencie nie różni się on koncepcyjnie od wytrenowanego zestawu **parametrów**; nie możemy twierdzić, że te hiperparametry będą zawsze najlepsze, ponieważ ucząc się na innych danych możemy dostać inne wartości średnio najlepszych hiperparametrów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "from itertools import product\n",
    "\n",
    "# dane\n",
    "from sklearn.datasets import fetch_mldata\n",
    "# splity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# modele\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "# metryki\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "# zawsze przed uczeniem/splitami proszę zrobić shuffle na danych!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czego będziemy używać:\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "* http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uwaga - modele będą się uczyły dłużej, niż zwykle. Warto dodać w kodzie dużo printów, żeby na bieżąco wiedzieć, co się dzieje.\n",
    "\n",
    "Na etapie pisania i testowania kodu można też zmniejszyć sobie losowo MNISTa np. do 5% przykładów, aby testy przebiegały szybciej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 1 [2 pkt]\n",
    "\n",
    "Napisać trzy generatory zestawów hiperparametrów:\n",
    "* generator grid search [robimy wspólnie za 0 pkt],\n",
    "* generator k elementów random grid search,\n",
    "* generator k elementów random search.\n",
    "\n",
    "Hiperparametry podajemy jako słownik {nazwa_hiperparametru: lista_wartości/rozkład}.\n",
    "\n",
    "Generator ma yieldować słowniki {nazwa_hiperparametru: wartość_hiperparametru}\n",
    "\n",
    "Rozkłady prawdopodobieństwa możecie Państwo podawać w dowolny sposób - np. jako pythonową funkcję, jako string z nazwą (a możliwe rozkłady zakodować na sztywno w funkcji zwracającej generatory) itp. Proponuję zastosować konwencję opisaną w następnej komórce [czy ktoś z Państwa ma inny pomysł, jak podawać rozkłady?].\n",
    "\n",
    "Losowe generatory mają przyjmować random_state i działać deterministycznie przy ustalonym random_state (należy ustawić w środku seed generatora liczb losowych na random_state).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tworzymy fabryki samplerów, każda fabryka parametryzuje pewną rodzinę rozkładów prawdopodobieństwa\n",
    "# sampler pamięta jeden konkretny rozkład prawdopodobieństwa\n",
    "# jedynym argumentem samplera jest obiekt numpy.random.RandomState, który oznaczamy rng\n",
    "# sampler zwraca jedną wartość ze swojego rozkładu\n",
    "# ponieważ rng pamięta swój stan, to możemy np. raz stworzyć go na początku i podawać w pętli\n",
    "\n",
    "# rodzina rozkładów jednostajnych na podanych listach elementów\n",
    "# sampler losuje z rozkładu jednostajnego na liście l\n",
    "def uniform_from_list(l):\n",
    "    def sampler(rng):\n",
    "        return l[rng.randint(0,len(l))]\n",
    "    return sampler\n",
    "\n",
    "# rodzina rozkładów jednostajnych na przedziałach liczb całkowitych\n",
    "# sampler losuje z rozkładu jednostajnego na podzbiorze liczb całkowitych od low (włącznie) do high (wyłącznie)\n",
    "def uniform_int_on_interval(low, high):\n",
    "    def sampler(rng):\n",
    "        return rng.randint(low,high)\n",
    "    return sampler\n",
    "\n",
    "# rodzina rozkładów jednostajnych na przedziałach\n",
    "# sampler losuje z rozkładu jednostajnego na przedziale [low, high]\n",
    "def uniform_on_interval(low, high):\n",
    "    def sampler(rng):\n",
    "        return rng.uniform(low, high)\n",
    "    return sampler\n",
    "\n",
    "# rodzina rozkładów jednostajnych na przedziałach w wykładniku potęgi liczby 10\n",
    "# sampler losuje liczbę alpha z rozkładu jednostajnego na przedziale [low, high], a następnie zwraca 10^alpha\n",
    "def log_uniform_on_interval(low, high):\n",
    "    def sampler(rng):\n",
    "        return 10.**rng.uniform(low, high)\n",
    "    return sampler\n",
    "\n",
    "# rodzina rozkładów gaussa\n",
    "# sampler losuje liczbę z rozkładu N(mean, std^2)\n",
    "def normal(mean, std):\n",
    "    def sampler(rng):\n",
    "        return rng.normal(loc=mean, scale=std)\n",
    "    return sampler\n",
    "\n",
    "# rodzina rozkładów gaussa w wykładniku potęgi liczby 10\n",
    "# sampler losuje liczbę alpha z rozkładu N(mean, std^2), a następnie zwraca 10^alpha\n",
    "def log_normal(mean, std):\n",
    "    def sampler(rng):\n",
    "        return 10**rng.normal(loc=mean, scale=std)\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tworzymy obiekt klasy RandomState\n",
      "Losujemy...\n",
      "-0.842587612353 0.000472401243122 0\n",
      "0.153322909413 543.807159801 -2\n",
      "-0.341758747593 1765.13251908 3\n",
      "-0.593715774857 8.32922889041 -1\n",
      "\n",
      "Resetujemy RandomState\n",
      "Losujemy...\n",
      "-0.842587612353 0.000472401243122 0\n",
      "0.153322909413 543.807159801 -2\n",
      "-0.341758747593 1765.13251908 3\n",
      "-0.593715774857 8.32922889041 -1\n",
      "\n",
      "Resetujemy RandomState\n",
      "Losujemy w innej kolejności, wypisujemy w starej...\n",
      "-1.50310753692 5.57429041087 4\n",
      "0.394467225601 0.000472401243122 0\n",
      "0.343807964283 543.807159801 0\n",
      "-0.775106170237 1765.13251908 4\n",
      "\n",
      "samplery współdzielą rng, kolejność losowania ma znaczenie!\n"
     ]
    }
   ],
   "source": [
    "print \"Tworzymy obiekt klasy RandomState\"\n",
    "rng = RandomState(743)\n",
    "s1 = normal(0.,1.)\n",
    "s2 = log_uniform_on_interval(-4.,4.)\n",
    "s3 = uniform_int_on_interval(-2,5)\n",
    "print \"Losujemy...\"\n",
    "for _ in xrange(4):\n",
    "    print s1(rng), s2(rng), s3(rng)\n",
    "\n",
    "print \"\"\n",
    "print \"Resetujemy RandomState\"\n",
    "rng = RandomState(743)\n",
    "s1 = normal(0.,1.)\n",
    "s2 = log_uniform_on_interval(-4.,4.)\n",
    "s3 = uniform_int_on_interval(-2,5)\n",
    "print \"Losujemy...\"\n",
    "for _ in xrange(4):\n",
    "    print s1(rng), s2(rng), s3(rng)\n",
    "\n",
    "print \"\"\n",
    "print \"Resetujemy RandomState\"\n",
    "rng = RandomState(743)\n",
    "s1 = normal(0.,1.)\n",
    "s2 = log_uniform_on_interval(-4.,4.)\n",
    "s3 = uniform_int_on_interval(-2,5)\n",
    "print \"Losujemy w innej kolejności, wypisujemy w starej...\"\n",
    "for _ in xrange(4):\n",
    "    _s3 = s3(rng)\n",
    "    _s2 = s2(rng)\n",
    "    _s1 = s1(rng)\n",
    "    print _s1, _s2, _s3\n",
    "\n",
    "print \"\"\n",
    "print \"samplery współdzielą rng, kolejność losowania ma znaczenie!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grid_search(grid):\n",
    "    (keys, values_grid) = zip(*grid.iteritems())\n",
    "    for values in product(*values_grid):\n",
    "        yield dict(zip(keys, values))\n",
    "\n",
    "def random_grid_search(grid, k=20, random_state=43):\n",
    "    rng = RandomState(random_state) # ustalamy jeden wspólny rng\n",
    "    (keys, values_grid) = zip(*grid.iteritems())\n",
    "    parameters_product = np.array(list(product(*values_grid)))\n",
    "    chosen = set()\n",
    "    while len(chosen)<k:\n",
    "        chosen.add(rng.randint(0, parameters_product.shape[0]))\n",
    "                   \n",
    "    for p in parameters_product[np.sort(np.array(list(chosen)))]:\n",
    "        yield dict(zip(keys, p))\n",
    "\n",
    "def random_search(grid, k=20, random_state=43):\n",
    "    rng = RandomState(random_state) # ustalamy jeden wspólny rng\n",
    "    (keys, samplers) = zip(*sorted(grid.iteritems())) # sortujemy klucze, kolejność samplowania jest ważna!\n",
    "    # wysamplować k zestawów hiperparametrów\n",
    "    # ...\n",
    "    for _ in xrange(k):\n",
    "        values = []\n",
    "        for s in samplers:\n",
    "            values.append(s(rng))\n",
    "        yield dict(zip(keys, values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1, 'gamma': 0.0001}\n",
      "{'C': 0.1, 'gamma': 0.0003}\n",
      "{'C': 0.1, 'gamma': 0.001}\n",
      "{'C': 1.0, 'gamma': 0.0001}\n",
      "{'C': 1.0, 'gamma': 0.0003}\n",
      "{'C': 1.0, 'gamma': 0.001}\n",
      "{'C': 10.0, 'gamma': 0.0001}\n",
      "{'C': 10.0, 'gamma': 0.0003}\n",
      "{'C': 10.0, 'gamma': 0.001}\n",
      "{'C': 100.0, 'gamma': 0.0001}\n",
      "{'C': 100.0, 'gamma': 0.0003}\n",
      "{'C': 100.0, 'gamma': 0.001}\n",
      "\n",
      "{'C': 0.10000000000000001, 'gamma': 0.0001}\n",
      "{'C': 0.10000000000000001, 'gamma': 0.00029999999999999997}\n",
      "{'C': 0.10000000000000001, 'gamma': 0.001}\n",
      "{'C': 1.0, 'gamma': 0.0001}\n",
      "{'C': 1.0, 'gamma': 0.00029999999999999997}\n",
      "{'C': 1.0, 'gamma': 0.001}\n",
      "{'C': 100.0, 'gamma': 0.00029999999999999997}\n",
      "{'C': 100.0, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# assert dla grid_search\n",
    "for d in grid_search({'C': [0.1, 1., 10., 100.], 'gamma': [0.0001, 0.0003, 0.001]}):\n",
    "    print d\n",
    "    \n",
    "print \"\"\n",
    "\n",
    "for d in random_grid_search({'C': [0.1, 1., 10., 100.], 'gamma': [0.0001, 0.0003, 0.001]}, k=8):\n",
    "    print d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'C': 0.1, 'gamma': 0.0001}<br/>\n",
    "{'C': 0.1, 'gamma': 0.0003}<br/>\n",
    "{'C': 0.1, 'gamma': 0.001}<br/>\n",
    "{'C': 1.0, 'gamma': 0.0001}<br/>\n",
    "{'C': 1.0, 'gamma': 0.0003}<br/>\n",
    "{'C': 1.0, 'gamma': 0.001}<br/>\n",
    "{'C': 10.0, 'gamma': 0.0001}<br/>\n",
    "{'C': 10.0, 'gamma': 0.0003}<br/>\n",
    "{'C': 10.0, 'gamma': 0.001}<br/>\n",
    "{'C': 100.0, 'gamma': 0.0001}<br/>\n",
    "{'C': 100.0, 'gamma': 0.0003}<br/>\n",
    "{'C': 100.0, 'gamma': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assert dla random_grid_search\n",
    "# ma wyjść tak samo, jak random_search, tylko z podanymi samplerami uniform_from_list\n",
    "# proszę sobie sprawdzić we własnym zakresie, my uwierzymy na słowo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.00014143134252521947, 'nb_sth': 9, 'gamma': 0.06090665392794814}\n",
      "{'C': 0.0025462216096392796, 'nb_sth': 13, 'gamma': 0.03271390558111398}\n",
      "{'C': 5.590022241698091e-05, 'nb_sth': 1, 'gamma': 0.004505286023886656}\n",
      "{'C': 1.950465371399982e-05, 'nb_sth': 13, 'gamma': 0.0733748296280283}\n",
      "{'C': 0.0002976304542098269, 'nb_sth': 13, 'gamma': 0.0011286104130902254}\n",
      "{'C': 3.705521747936987e-05, 'nb_sth': 22, 'gamma': 0.08666486408992002}\n",
      "{'C': 31.420766577465653, 'nb_sth': 22, 'gamma': 0.05808772319264447}\n",
      "{'C': 5.842880444285076e-05, 'nb_sth': 1, 'gamma': 0.08432246942297046}\n",
      "{'C': 16.058920736060898, 'nb_sth': 22, 'gamma': 0.04037700398666926}\n",
      "{'C': 35065.59811614318, 'nb_sth': 22, 'gamma': 0.04457583608397189}\n",
      "{'C': 6.683446823678905e-05, 'nb_sth': 13, 'gamma': 0.08970985799815344}\n",
      "{'C': 965.8767858660282, 'nb_sth': 1, 'gamma': 0.0332329385020804}\n",
      "{'C': 0.28068153065434537, 'nb_sth': 1, 'gamma': 0.08247676837174123}\n",
      "{'C': 12207.52376193616, 'nb_sth': 13, 'gamma': 0.09101530757801567}\n",
      "{'C': 0.00015294037493259258, 'nb_sth': 9, 'gamma': 0.09375550594407336}\n"
     ]
    }
   ],
   "source": [
    "for d in random_search(\n",
    "        {'C': log_uniform_on_interval(-5., 5.),\n",
    "         'gamma': uniform_on_interval(0.,0.1),\n",
    "         'nb_sth': uniform_from_list([1,4,9,13,22])},\n",
    "        k=15,\n",
    "        random_state=43):\n",
    "    print d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'C': 0.00014143134252521947, 'nb_sth': 9, 'gamma': 0.06090665392794814}<br/>\n",
    "{'C': 0.0025462216096392796, 'nb_sth': 13, 'gamma': 0.03271390558111398}<br/>\n",
    "{'C': 5.590022241698091e-05, 'nb_sth': 1, 'gamma': 0.004505286023886656}<br/>\n",
    "{'C': 1.950465371399982e-05, 'nb_sth': 13, 'gamma': 0.0733748296280283}<br/>\n",
    "{'C': 0.0002976304542098269, 'nb_sth': 13, 'gamma': 0.0011286104130902254}<br/>\n",
    "{'C': 3.705521747936987e-05, 'nb_sth': 22, 'gamma': 0.08666486408992002}<br/>\n",
    "{'C': 31.420766577465653, 'nb_sth': 22, 'gamma': 0.05808772319264447}<br/>\n",
    "{'C': 5.842880444285076e-05, 'nb_sth': 1, 'gamma': 0.08432246942297046}<br/>\n",
    "{'C': 16.058920736060898, 'nb_sth': 22, 'gamma': 0.04037700398666926}<br/>\n",
    "{'C': 35065.59811614318, 'nb_sth': 22, 'gamma': 0.04457583608397189}<br/>\n",
    "{'C': 6.683446823678905e-05, 'nb_sth': 13, 'gamma': 0.08970985799815344}<br/>\n",
    "{'C': 965.8767858660282, 'nb_sth': 1, 'gamma': 0.0332329385020804}<br/>\n",
    "{'C': 0.28068153065434537, 'nb_sth': 1, 'gamma': 0.08247676837174123}<br/>\n",
    "{'C': 12207.52376193616, 'nb_sth': 13, 'gamma': 0.09101530757801567}<br/>\n",
    "{'C': 0.00015294037493259258, 'nb_sth': 9, 'gamma': 0.09375550594407336}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 2 [0 pkt]\n",
    "\n",
    "Zaimplementować algorytm model selection z użyciem cross validation (StratifiedKFold w sklearn) zgodnie z opisem w sekcji \"Podsumowanie\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def skf_model_selection(Model, hyperparams_generator, X, y, metric, n_splits, random_state):\n",
    "    \"\"\"\n",
    "    Model - klasa modelu\n",
    "    hyperparams_generator - generator hiperparametrów\n",
    "    X,y - dane i etykiety\n",
    "    metric - funkcja o sygnaturze metric(y_true, y_pred), która ocenia skuteczność nauczonego modelu\n",
    "    n_splits - liczba splitów/foldów w cross validation\n",
    "    random_state - używany wszędzie tam, gdzie trzeba\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []\n",
    "    tested_hyperparams = []\n",
    "    \n",
    "    # Aby wytrenować metamodel:\n",
    "    # 1. Dzielimy n_splits (X,y) na (X_train,y_train), (X_valid,y_valid).\n",
    "        # dla przejrzystości algorytmu zapamiętujemy indeksy podziału w liście indices\n",
    "        # w normalnym kodzie trzeba np. wielokrotnie stworzyć generator skf głębiej w pętli\n",
    "    indices = []\n",
    "    skf = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "    for train_index, valid_index in skf.split(X,y):\n",
    "        indices.append((train_index, valid_index))\n",
    "    # 2. Dla każdego zestawu hiperparametrów:\n",
    "    for hyperparams in hyperparams_generator:\n",
    "        # 1. Dla każdego podziału (X_train,y_train), (X_valid,y_valid):\n",
    "        scores.append([])\n",
    "        print \"Testujemy hiperparametry:\", hyperparams\n",
    "        for i, (train_index, valid_index) in enumerate(indices):\n",
    "            X_train, y_train = X[train_index], y[train_index]\n",
    "            X_valid, y_valid = X[valid_index], y[valid_index]\n",
    "            # 1. Tworzymy model = Model(zestaw_hiperparametrów).\n",
    "            model = Model(**hyperparams)\n",
    "            # 2. Uczymy model na (X_train,y_train), testujemy na (X_valid,y_valid) i otrzymujemy score.\n",
    "            print \"Uczymy model - split \" + str(i+1) + \"/\" + str(n_splits) + \"...\"\n",
    "            model.fit(X_train, y_train)\n",
    "            print \"Liczymy score...\"\n",
    "            score = metric(y_valid, model.predict(X_valid)) # w zależności od metryki czasem używa się model.predict_proba, tu zakładamy, że metryka jest typu 'accuracy'\n",
    "            print \"Train score:\", metric(y_train, model.predict(X_train))\n",
    "            print \"Valid score:\", score\n",
    "            # 3. (zestaw_hiperparametrów, score) zapisujemy w tabelce.\n",
    "            scores[-1].append(score)\n",
    "        tested_hyperparams.append(hyperparams)\n",
    "    # 3. Dla każdego zestawu hiperparametrów mamy trzy różne score z trzech podziałów - uśredniamy je.\n",
    "    avg_scores = [np.mean(scores_for_single_hyperparams) for scores_for_single_hyperparams in scores]\n",
    "    # 4. Wybieramy średnio najlepszy zestaw hiperparametrów.\n",
    "    best_hyperparams = tested_hyperparams[np.argmax(avg_scores)]\n",
    "    print \"Najlepszy zestaw hiperparametrów:\", best_hyperparams\n",
    "    # 5. Tworzymy model = Model(średnio_najlepszy_zestaw_hiperparametrów) i uczymy na całym (X,y).\n",
    "    best_model = Model(**best_hyperparams)\n",
    "    print \"Uczymy najlepszy model...\"\n",
    "    best_model.fit(X, y)\n",
    "    # Uwaga - teraz mamy tylko jeden średnio_najlepszy_zestaw_hiperparametrów i jeden model.\n",
    "    return best_model, best_hyperparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testujemy hiperparametry: {'n_neighbors': 2, 'leaf_size': 20}\n",
      "Uczymy model - split 1/2...\n",
      "Liczymy score...\n",
      "Train score: 0.954636424283\n",
      "Valid score: 0.893404397069\n",
      "Uczymy model - split 2/2...\n",
      "Liczymy score...\n",
      "Train score: 0.950699533644\n",
      "Valid score: 0.886591060707\n",
      "Testujemy hiperparametry: {'n_neighbors': 2, 'leaf_size': 30}\n",
      "Uczymy model - split 1/2...\n",
      "Liczymy score...\n",
      "Train score: 0.954636424283\n",
      "Valid score: 0.893404397069\n",
      "Uczymy model - split 2/2...\n",
      "Liczymy score...\n",
      "Train score: 0.950699533644\n",
      "Valid score: 0.886591060707\n",
      "Testujemy hiperparametry: {'n_neighbors': 2, 'leaf_size': 40}\n",
      "Uczymy model - split 1/2...\n",
      "Liczymy score...\n",
      "Train score: 0.954636424283\n",
      "Valid score: 0.893404397069\n",
      "Uczymy model - split 2/2...\n",
      "Liczymy score...\n",
      "Train score: 0.950699533644\n",
      "Valid score: 0.886591060707\n",
      "Testujemy hiperparametry: {'n_neighbors': 5, 'leaf_size': 20}\n",
      "Uczymy model - split 1/2...\n",
      "Liczymy score...\n",
      "Train score: 0.939959973316\n",
      "Valid score: 0.90872751499\n",
      "Uczymy model - split 2/2...\n",
      "Liczymy score...\n",
      "Train score: 0.942038640906\n",
      "Valid score: 0.904603068712\n",
      "Testujemy hiperparametry: {'n_neighbors': 5, 'leaf_size': 30}\n",
      "Uczymy model - split 1/2...\n",
      "Liczymy score...\n",
      "Train score: 0.939959973316\n",
      "Valid score: 0.90872751499\n",
      "Uczymy model - split 2/2...\n",
      "Liczymy score...\n",
      "Train score: 0.942038640906\n",
      "Valid score: 0.904603068712\n",
      "Testujemy hiperparametry: {'n_neighbors': 5, 'leaf_size': 40}\n",
      "Uczymy model - split 1/2...\n",
      "Liczymy score...\n",
      "Train score: 0.939959973316\n",
      "Valid score: 0.90872751499\n",
      "Uczymy model - split 2/2...\n",
      "Liczymy score...\n",
      "Train score: 0.942038640906\n",
      "Valid score: 0.904603068712\n",
      "Testujemy hiperparametry: {'n_neighbors': 8, 'leaf_size': 20}\n",
      "Uczymy model - split 1/2...\n",
      "Liczymy score...\n",
      "Train score: 0.917278185457\n",
      "Valid score: 0.898734177215\n",
      "Uczymy model - split 2/2...\n",
      "Liczymy score...\n",
      "Train score: 0.917388407728\n",
      "Valid score: 0.889259506338\n",
      "Testujemy hiperparametry: {'n_neighbors': 8, 'leaf_size': 30}\n",
      "Uczymy model - split 1/2...\n",
      "Liczymy score...\n",
      "Train score: 0.917278185457\n",
      "Valid score: 0.898734177215\n",
      "Uczymy model - split 2/2...\n",
      "Liczymy score...\n",
      "Train score: 0.917388407728\n",
      "Valid score: 0.889259506338\n",
      "Testujemy hiperparametry: {'n_neighbors': 8, 'leaf_size': 40}\n",
      "Uczymy model - split 1/2...\n",
      "Liczymy score...\n",
      "Train score: 0.917278185457\n",
      "Valid score: 0.898734177215\n",
      "Uczymy model - split 2/2...\n",
      "Liczymy score...\n",
      "Train score: 0.917388407728\n",
      "Valid score: 0.889259506338\n",
      "Najlepszy zestaw hiperparametrów: {'n_neighbors': 5, 'leaf_size': 20}\n",
      "Uczymy najlepszy model...\n"
     ]
    }
   ],
   "source": [
    "# przetestujmy powyższe na losowym podzbiorze MNISTa\n",
    "\n",
    "idx = np.random.RandomState(743).permutation(len(y))\n",
    "_X = X[idx[:3000]]\n",
    "_y = y[idx[:3000]]\n",
    "\n",
    "best_model, best_hyperparams = skf_model_selection(\n",
    "    Model=KNeighborsClassifier,\n",
    "    hyperparams_generator=grid_search({'n_neighbors': [2,5,8], 'leaf_size': [20,30,40]}),\n",
    "    X=_X, y=_y,\n",
    "    metric=accuracy_score,\n",
    "    n_splits=2,\n",
    "    random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 3 [4 pkt]\n",
    "\n",
    "Zaimplementować algorytm model selection z użyciem podwójnej cross validation (StratifiedKFold w sklearn) zgodnie z opisem w sekcji \"Podsumowanie\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def double_skf_model_evaluation(Model, generator_function, generator_function_kwargs, X, y, metric, selection_n_splits, evaluation_n_splits, random_state):\n",
    "    \"\"\"\n",
    "    Model - klasa modelu\n",
    "    generator_function, generator_function_kwargs - chcemy zrobić coś w stylu:\n",
    "        for hyperparams in generator_function(**generator_function_kwargs):\n",
    "            ...\n",
    "        nie podajemy wprost generatora, bo trzeba go użyć wielokrotnie, a generatorów (chyba?) nie da się kopiować\n",
    "    X,y - dane i etykiety\n",
    "    metric - funkcja o sygnaturze metric(y_true, y_pred), która ocenia skuteczność nauczonego modelu\n",
    "    selection/evaluation_n_splits - liczba splitów/foldów w odpowiednim cross validation\n",
    "    random_state - używany wszędzie tam, gdzie trzeba\n",
    "    \"\"\"\n",
    "    # 1. Dzielimy evaluation_n_splits razy (X,y) na (X_train,y_train), (X_test,y_test).\n",
    "    # 2. Dla każdego takiego podziału:\n",
    "        # 1. Dzielimy selection_n_splits razy (X_train,y_train) na (X_train2,y_train2), (X_valid,y_valid).\n",
    "        # 2. Dla każdego zestawu hiperparametrów:\n",
    "            # 1. Dla każdego podziału (X_train2,y_train2), (X_valid,y_valid):\n",
    "                # 1. Tworzymy model = Model(zestaw_hiperparametrów).\n",
    "                # 2. Uczymy model na (X_train2,y_train2), testujemy na (X_valid,y_valid) i otrzymujemy score.\n",
    "                # 3. (zestaw_hiperparametrów, score) zapisujemy w tabelce.\n",
    "        # 3. Dla każdego zestawu hiperparametrów mamy trzy różne score z trzech podziałów - uśredniamy je.\n",
    "        # 4. Wybieramy średnio najlepszy zestaw hiperparametrów.\n",
    "        # 5. Tworzymy model = Model(średnio_najlepszy_zestaw_hiperparametrów) i uczymy na całym (X_train,y_train).\n",
    "        # 6. Testujemy model na (X_test,y_test) i otrzymujemy evaluation_score, zapisujemy go.\n",
    "    # 3. Uśredniamy trzy evaluation_score, zwracamy średnią jako ostateczny score Modelu.\n",
    "    \n",
    "    indicies = []\n",
    "    evaluation_scores = []\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=evaluation_n_splits, random_state=random_state, shuffle=True)\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        indicies.append((train_index, test_index))\n",
    "        \n",
    "    for i, (train_index, test_index) in enumerate(indicies):\n",
    "        \n",
    "        print \"Starting testing new train/test indicies\"\n",
    "        \n",
    "        selection_indicies = []\n",
    "        selection_scores = []\n",
    "        tested_hyperparams = []\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=selection_n_splits, random_state=random_state, shuffle=True)\n",
    "        \n",
    "        X_train = X[train_index]\n",
    "        y_train = y[train_index]\n",
    "        \n",
    "        for (train2_index, valid_index) in skf.split(X_train, y_train):\n",
    "            selection_indicies.append((train2_index, valid_index))\n",
    "            \n",
    "        for hyperparams in generator_function(generator_function_kwargs):\n",
    "            \n",
    "            print \"Testing hyperparams\", hyperparams\n",
    "            \n",
    "            selection_scores.append([])\n",
    "            for i, (train2_index, valid_index) in enumerate(selection_indicies):\n",
    "                    \n",
    "                print \".\" ,\n",
    "                \n",
    "                X_train2, y_train2 = X_train[train2_index], y_train[train2_index]\n",
    "                X_valid, y_valid = X_train[valid_index], y_train[valid_index]\n",
    "\n",
    "                model = Model(**hyperparams)\n",
    "\n",
    "                model.fit(X_train2, y_train2)\n",
    "                score = metric(y_valid, model.predict(X_valid))\n",
    "\n",
    "                selection_scores[-1].append(score)\n",
    "                \n",
    "            tested_hyperparams.append(hyperparams)\n",
    "                \n",
    "            print \" score: \", np.mean(selection_scores[-1])\n",
    "\n",
    "        avg_scores = [np.mean(scores_for_singe_params) for scores_for_singe_params in selection_scores]\n",
    "\n",
    "        best_hyperparams = tested_hyperparams[np.argmax(avg_scores)]\n",
    "\n",
    "        best_model = Model(**best_hyperparams)\n",
    "\n",
    "        best_model.fit(X_train, y_train)\n",
    "        \n",
    "        X_test = X[test_index]\n",
    "        y_test = y[test_index]\n",
    "        evaluation_scores.append(metric(y_test, model.predict(X_test)))\n",
    "        \n",
    "        \"Done testing train/test indicies. Score: \", metric(y_test, model.predict(X_test))\n",
    "        \n",
    "    print evaluation_scores\n",
    "    return np.mean(evaluation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting testing new train/test indicies\n",
      "Testing hyperparams {'n_neighbors': 2, 'leaf_size': 20}\n",
      ". .  score:  0.872892403484\n",
      "Testing hyperparams {'n_neighbors': 2, 'leaf_size': 30}\n",
      ". .  score:  0.872892403484\n",
      "Testing hyperparams {'n_neighbors': 2, 'leaf_size': 40}\n",
      ". .  score:  0.872892403484\n",
      "Testing hyperparams {'n_neighbors': 5, 'leaf_size': 20}\n",
      ". .  score:  0.891393962106\n",
      "Testing hyperparams {'n_neighbors': 5, 'leaf_size': 30}\n",
      ". .  score:  0.891393962106\n",
      "Testing hyperparams {'n_neighbors': 5, 'leaf_size': 40}\n",
      ". .  score:  0.891393962106\n",
      "Testing hyperparams {'n_neighbors': 8, 'leaf_size': 20}\n",
      ". .  score:  0.883381913974\n",
      "Testing hyperparams {'n_neighbors': 8, 'leaf_size': 30}\n",
      ". .  score:  0.883381913974\n",
      "Testing hyperparams {'n_neighbors': 8, 'leaf_size': 40}\n",
      ". .  score:  0.883381913974\n",
      "Starting testing new train/test indicies\n",
      "Testing hyperparams {'n_neighbors': 2, 'leaf_size': 20}\n",
      ". .  score:  0.864455141873\n",
      "Testing hyperparams {'n_neighbors': 2, 'leaf_size': 30}\n",
      ". .  score:  0.864455141873\n",
      "Testing hyperparams {'n_neighbors': 2, 'leaf_size': 40}\n",
      ". .  score:  0.864455141873\n",
      "Testing hyperparams {'n_neighbors': 5, 'leaf_size': 20}\n",
      ". .  score:  0.87643869733\n",
      "Testing hyperparams {'n_neighbors': 5, 'leaf_size': 30}\n",
      ". .  score:  0.87643869733\n",
      "Testing hyperparams {'n_neighbors': 5, 'leaf_size': 40}\n",
      ". .  score:  0.87643869733\n",
      "Testing hyperparams {'n_neighbors': 8, 'leaf_size': 20}\n",
      ". .  score:  0.864430116697\n",
      "Testing hyperparams {'n_neighbors': 8, 'leaf_size': 30}\n",
      ". .  score:  0.864430116697\n",
      "Testing hyperparams {'n_neighbors': 8, 'leaf_size': 40}\n",
      ". .  score:  0.864430116697\n",
      "Starting testing new train/test indicies\n",
      "Testing hyperparams {'n_neighbors': 2, 'leaf_size': 20}\n",
      ". .  score:  0.874191178083\n",
      "Testing hyperparams {'n_neighbors': 2, 'leaf_size': 30}\n",
      ". .  score:  0.874191178083\n",
      "Testing hyperparams {'n_neighbors': 2, 'leaf_size': 40}\n",
      ". .  score:  0.874191178083\n",
      "Testing hyperparams {'n_neighbors': 5, 'leaf_size': 20}\n",
      ". .  score:  0.891165222004\n",
      "Testing hyperparams {'n_neighbors': 5, 'leaf_size': 30}\n",
      ". .  score:  0.891165222004\n",
      "Testing hyperparams {'n_neighbors': 5, 'leaf_size': 40}\n",
      ". .  score:  0.891165222004\n",
      "Testing hyperparams {'n_neighbors': 8, 'leaf_size': 20}\n",
      ". .  score:  0.884675205034\n",
      "Testing hyperparams {'n_neighbors': 8, 'leaf_size': 30}\n",
      ". .  score:  0.884675205034\n",
      "Testing hyperparams {'n_neighbors': 8, 'leaf_size': 40}\n",
      ". .  score:  0.884675205034\n",
      "[0.88023952095808389, 0.88511488511488512, 0.85356068204613844]\n",
      "0.87297169604\n"
     ]
    }
   ],
   "source": [
    "mean_score = double_skf_model_evaluation(\n",
    "    Model=KNeighborsClassifier,\n",
    "    generator_function=grid_search,\n",
    "    generator_function_kwargs=({'n_neighbors': [2,5,8], 'leaf_size': [20,30,40]}),\n",
    "    X=_X, y=_y,\n",
    "    metric=accuracy_score,\n",
    "    selection_n_splits=2,\n",
    "    evaluation_n_splits=3,\n",
    "    random_state=43)\n",
    "\n",
    "print mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 4 [2 pkt]\n",
    "\n",
    "Przy użyciu powyższych funkcji wytrenować porządnie na danych MNIST jeden (dowolnie wybrany) z poniższych modelów:\n",
    "* LinearSVM,\n",
    "* SVM,\n",
    "* RandomForest,\n",
    "* KNN.\n",
    "\n",
    "Zwrócić model i jego estymowany score.\n",
    "\n",
    "[Możemy wspólnie zastanowić się nad sensownym doborem gridów/rozkładów na hiperparametrach.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poly\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 4, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 4, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 4, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 16, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 16, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 16, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 17, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 17, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 17, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 21, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 21, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 21, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 30, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 30, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 30, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 32, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 32, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 32, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 34, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 34, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 34, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 35, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 35, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 35, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 36, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 36, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 36, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 47, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 47, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 47, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 49, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 49, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 49, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 50, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 50, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 50, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 51, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 51, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 51, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 58, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 58, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 58, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 59, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 59, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 59, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 60, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 60, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 60, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 64, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 64, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 64, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 65, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 65, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 65, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 66, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 66, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 66, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 68, 'degree': 1}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 68, 'degree': 2}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998800239952\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998799879988\n",
      "Testujemy hiperparametry: {'kernel': 'poly', 'C': 68, 'degree': 3}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.997599759976\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.99699969997\n",
      "linear\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 4}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 16}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 17}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 21}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 30}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 32}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 34}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 35}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 36}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 47}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 49}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 50}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 51}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 58}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 59}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 60}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 64}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 65}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 66}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Testujemy hiperparametry: {'kernel': 'linear', 'C': 68}\n",
      "Uczymy model - split 1/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998200359928\n",
      "Uczymy model - split 2/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.999099909991\n",
      "Uczymy model - split 3/3...\n",
      "Liczymy score...\n",
      "Train score: 1.0\n",
      "Valid score: 0.998199819982\n",
      "Najlepszy zestaw hiperparametrów: {'kernel': 'poly', 'C': 4, 'degree': 2}\n",
      "Uczymy najlepszy model...\n",
      "{'kernel': 'poly', 'C': 4, 'degree': 2}\n"
     ]
    }
   ],
   "source": [
    "def my_hyperparams_generator(list_for_kernels, generator_for_C_args, generator_for_degree_args):\n",
    "    \n",
    "    for kernel in list_for_kernels:\n",
    "        print kernel\n",
    "        for C in random_grid_search(*generator_for_C_args):\n",
    "#             print dict(zip(C.keys() + {'kernel': kernel}.keys(), C.values() + [2]))\n",
    "            if (kernel=='poly'):\n",
    "                for degree in grid_search(generator_for_degree_args):\n",
    "                    yield dict(zip(['kernel', 'C', 'degree'], [kernel] + C.values() + degree.values()))\n",
    "            else:\n",
    "                yield dict(zip(['kernel', 'C'], [kernel] + C.values()))\n",
    "            \n",
    "mhg = my_hyperparams_generator(\n",
    "    list_for_kernels = ['poly', 'linear'],\n",
    "    generator_for_C_args = ({'C': range(0, 70)}, 20),\n",
    "    generator_for_degree_args = ({'degree': [1, 2, 3]})\n",
    ")\n",
    "\n",
    "\n",
    "best_mode_full, best_hyperparams = skf_model_selection(\n",
    "    Model=SVC,\n",
    "    hyperparams_generator = mhg,\n",
    "    X = X[:10000],\n",
    "    y = y[:10000],\n",
    "    metric = accuracy_score,\n",
    "    n_splits = 3,\n",
    "    random_state = 43\n",
    ")\n",
    "\n",
    "print best_hyperparams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score on best hyperparams:  Starting testing new train/test indicies\n",
      "Testing hyperparams {'kernel': 'poly', 'C': 4, 'degree': 2}\n",
      ". .  score:  0.972162721596\n",
      "Starting testing new train/test indicies\n",
      "Testing hyperparams {'kernel': 'poly', 'C': 4, 'degree': 2}\n",
      ". .  score:  0.973278709182\n",
      "Starting testing new train/test indicies\n",
      "Testing hyperparams {'kernel': 'poly', 'C': 4, 'degree': 2}\n",
      ". .  score:  0.972165579519\n",
      "[0.97214604045251973, 0.97239960570865303, 0.97141142685697146]\n",
      "0.971985691006\n"
     ]
    }
   ],
   "source": [
    "def one_hyperparams_generator(params):\n",
    "    yield params\n",
    "\n",
    "print \"Score on best hyperparams: \", double_skf_model_evaluation(\n",
    "    Model=SVC,\n",
    "    generator_function = one_hyperparams_generator,\n",
    "    generator_function_kwargs = best_hyperparams,\n",
    "    X = X, y = y,\n",
    "    metric = accuracy_score,\n",
    "    selection_n_splits = 2,\n",
    "    evaluation_n_splits = 3,\n",
    "    random_state = 43\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "Model zawsze nauczy się lepiej na większym zbiorze danych. Przez data augmentation rozumiemy cały zbiór technik, które pozwalają \"sztucznie\" niskim kosztem powiększyć dataset. Oczywiście musimy znać etykiety nowowygenerowanych danych. Zazwyczaj działamy w następujący sposób: definiujemy operację, którą możemy zadziałać na pojedynczy $\\mathbf{x}$ bez zmiany odpowiadającego mu $y$, a następnie generujemy z jednego $\\mathbf{x}$ dużo wersji i każdej przypisujemy to samo $y$.\n",
    "\n",
    "Kilka prostych przykładów:\n",
    "* rozpoznawanie mowy ($\\mathbf{x}$ - zapis dźwiękowy, $y$ - tekst wypowiadany przez lektora) - zmieniamy szybkość mówienia, zmieniamy wysokość dźwięku itp.\n",
    "* klasyfikacja obrazków - obracamy obrazki, odbijamy symetrycznie, dodajemy szum, zmieniamy nieco paletę barw itp.\n",
    "\n",
    "Spróbujmy zobaczyć dla przykładu, co się stanie, gdy dodamy gaussowski szum do MNISTa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "idx = np.random.permutation(len(y))\n",
    "X_3K = X[idx[:3000]]\n",
    "y_3K = y[idx[:3000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting KNN (no augmentation)\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.924242424242\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_3K, y_3K, test_size=0.33, random_state=43)\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "print \"fitting KNN (no augmentation)\"\n",
    "model.fit(X_train, y_train)\n",
    "print \"train accuracy:\", accuracy_score(y_train, model.predict(X_train))\n",
    "print \"test accuracy:\", accuracy_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting data...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADi9JREFUeJzt3X+MVPW5x/HP0xXcxFaDrW4IsG6Jv1JNLg0bNYqmFwqx\nphH7jyn+CM0ldxsEFHP/8Mf94xobtd7ctmlirEIk3Va0vYm/oN5IKhrFeKOAoaJYEM2WslngGqpA\n1ODK0z/mbO+KO98Z5pyZM7vP+5VsduY8c855MtnPnjPzPTNfc3cBiOcrZTcAoByEHwiK8ANBEX4g\nKMIPBEX4gaAIPxAU4QeCIvxAUCe1cmdmxuWEQJO5u9XzuFxHfjO70sx2mtluM7s9z7YAtJY1em2/\nmXVI2iVpvqS9kjZLWuTuOxLrcOQHmqwVR/6LJO129/fd/aik30lamGN7AFooT/inSfrrqPt7s2Vf\nYGZ9ZrbFzLbk2BeAgjX9DT93XyVplcRpP9BO8hz5ByXNGHV/erYMwDiQJ/ybJZ1jZt80s8mSfihp\nXTFtAWi2hk/73X3YzJZL2iCpQ9Iad3+7sM4ANFXDQ30N7YzX/EDTteQiHwDjF+EHgiL8QFCEHwiK\n8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8I\nivADQbV0iu52dscddyTr9913X4s6aS8vvfRSsn7FFVc0vO2bb745WX/ggQeS9VZ+8/RExJEfCIrw\nA0ERfiAowg8ERfiBoAg/EBThB4LKNUuvmQ1IOizpc0nD7t5b4/EMzLaZmTNnJusvvvhisj59+vQi\n2/mCZcuWJesPPfRQ0/Y9ntU7S28RF/n8s7t/UMB2ALQQp/1AUHnD75KeN7OtZtZXREMAWiPvaf8c\ndx80szMl/dHM/uzuL49+QPZPgX8MQJvJdeR398Hs9wFJT0m6aIzHrHL33lpvBgJorYbDb2anmNnX\nRm5LWiDpraIaA9BceU77uyQ9ZWYj23nM3Z8rpCsATddw+N39fUn/VGAvKMGMGTOS9bzj+EePHq1a\n6+joSK57ySWXJOuM8+fDUB8QFOEHgiL8QFCEHwiK8ANBEX4gKL66e4KbPHlysn711Vc3df8rVqyo\nWrvllluS61588cXJend3d7K+Z8+eZD06jvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFSur+4+4Z3x\n1d1NcfLJJ1et3Xvvvcl1V65cmWvf27dvT9bnzZtXtfb0008n17300kuT9V27diXrqY8Ef/TRR8l1\nx7N6v7qbIz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBMU4/wQwf/78qrXnnss3lcLHH3+crC9dujRZ\nf/TRR6vWenp6kuu++uqryXpXV1eyftNNN1WtPfzww8l1xzPG+QEkEX4gKMIPBEX4gaAIPxAU4QeC\nIvxAUDXH+c1sjaTvSzrg7hdmy06X9HtJPZIGJF3r7n+ruTPG+RvS2dmZrK9fv75qbe7cubn2/dhj\njyXrN954Y67tp2zatClZr/V5/927d1etzZ49O7nukSNHkvV2VuQ4/68lXXncstslbXT3cyRtzO4D\nGEdqht/dX5Z08LjFCyX1Z7f7JV1TcF8AmqzR1/xd7j6U3d4nKX2dJYC2k3uuPnf31Gt5M+uT1Jd3\nPwCK1eiRf7+ZTZWk7PeBag9091Xu3uvuvQ3uC0ATNBr+dZIWZ7cXS3qmmHYAtErN8JvZ45L+V9J5\nZrbXzJZI+qmk+Wb2rqTvZvcBjCM1X/O7+6IqpepfyI4TUmsc//7770/W847lp5T5uffVq1cn67XG\n+c8+++yqteuvvz657kT+vP8IrvADgiL8QFCEHwiK8ANBEX4gKMIPBJX78l7k193dnawvX768afve\nsGFDsr5jx46m7btM5557btktlI4jPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ExTh/G7juuuuatu1a\n4/Q33HBDsn7w4PHf3YqJgiM/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRVc4ruQncWdIru2267LVlf\nuXJlsn7mmWc2vO8yp9jOq6srPQXktm3bkvXU8zY4OJhc97zzzkvWP/nkk2S9TEVO0Q1gAiL8QFCE\nHwiK8ANBEX4gKMIPBEX4gaBqfp7fzNZI+r6kA+5+YbbsLkn/Kun/sofd6e7/06wmx7vUVNFSvnF8\nSXr22Wer1lasWJFr22WaNGlSsm5W13D2mKZNm5asd3R0NLzt8aKeI/+vJV05xvJfuPus7IfgA+NM\nzfC7+8uS+DoXYILJ85p/hZm9aWZrzGxKYR0BaIlGw/8rSTMlzZI0JOln1R5oZn1mtsXMtjS4LwBN\n0FD43X2/u3/u7sckrZZ0UeKxq9y91917G20SQPEaCr+ZTR119weS3iqmHQCtUs9Q3+OSviPpG2a2\nV9J/SPqOmc2S5JIGJP24iT0CaIKa4Xf3RWMsfqQJvYxbnZ2dyfpZZ52Va/ubN29O1u+5556qtQ8/\n/DDXvss0d+7cZP2MM85oeNvvvfdesj48PNzwtscLrvADgiL8QFCEHwiK8ANBEX4gKMIPBMUU3QWY\nMiX90YZ58+bl2v7rr7+erL/22mu5th/R+vXrk/VPP/20RZ2UhyM/EBThB4Ii/EBQhB8IivADQRF+\nICjCDwTFOH8bOHz4cLK+du3aFnXSWj09Pcl6ranNkQ9HfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8I\ninH+Alx++eW51t+3b1+y3s6f1+/u7k7WFyxYULV26623Jtc9//zzG+oJ9eHIDwRF+IGgCD8QFOEH\ngiL8QFCEHwiK8ANB1RznN7MZkn4jqUuSS1rl7r80s9Ml/V5Sj6QBSde6+9+a12r72rRpU671Tzvt\ntGR96dKlyfrWrVur1mbPnp1ct9b04ZdddlmyXqv3Cy64IFlP2blzZ7I+NDSUrKeuIxgYGGikpQml\nniP/sKR/c/dvSbpE0jIz+5ak2yVtdPdzJG3M7gMYJ2qG392H3P2N7PZhSe9ImiZpoaT+7GH9kq5p\nVpMAindCr/nNrEfStyW9JqnL3UfOu/ap8rIAwDhR97X9ZvZVSU9IWunuh8zsHzV3dzPzKuv1SerL\n2yiAYtV15DezSaoEf627P5kt3m9mU7P6VEkHxlrX3Ve5e6+79xbRMIBi1Ay/VQ7xj0h6x91/Pqq0\nTtLi7PZiSc8U3x6AZjH3Mc/W//8BZnMkbZK0XdKxbPGdqrzu/29J3ZL+ospQ38Ea20rvbJw69dRT\nk/VXXnklWc8zHCalp5Pu7OzMte0yLV++PFlfs2ZNsv7ZZ59VrR07dqxqbbxzd6v9qDpe87v7K5Kq\nbSzfxPMASsMVfkBQhB8IivADQRF+ICjCDwRF+IGg+OruAhw6dChZr/XV23nH+dt5LL+/v79q7e67\n706uu2fPnmR9Io/VtwJHfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8Iqubn+Qvd2QT9PH8tJ52Uvpxi\nyZIlyfqDDz6YrKe+OrzW9OFr165N1l944YVc6w8PD1ettfJvL5J6P8/PkR8IivADQRF+ICjCDwRF\n+IGgCD8QFOEHgmKcH5hgGOcHkET4gaAIPxAU4QeCIvxAUIQfCIrwA0HVDL+ZzTCzF81sh5m9bWa3\nZMvvMrNBM9uW/VzV/HYBFKXmRT5mNlXSVHd/w8y+JmmrpGskXSvpiLv/V9074yIfoOnqvcin5ow9\n7j4kaSi7fdjM3pE0LV97AMp2Qq/5zaxH0rcljcw/tcLM3jSzNWY2pco6fWa2xcy25OoUQKHqvrbf\nzL4q6SVJ97j7k2bWJekDSS7pJ6q8NPiXGtvgtB9osnpP++sKv5lNkvQHSRvc/edj1Hsk/cHdL6yx\nHcIPNFlhH+wxM5P0iKR3Rgc/eyNwxA8kvXWiTQIoTz3v9s+RtEnSdkkjcyLfKWmRpFmqnPYPSPpx\n9uZgalsc+YEmK/S0vyiEH2g+Ps8PIInwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+\nICjCDwRF+IGgCD8QVM0v8CzYB5L+Mur+N7Jl7ahde2vXviR6a1SRvZ1V7wNb+nn+L+3cbIu795bW\nQEK79taufUn01qiyeuO0HwiK8ANBlR3+VSXvP6Vde2vXviR6a1QpvZX6mh9Aeco+8gMoSSnhN7Mr\nzWynme02s9vL6KEaMxsws+3ZzMOlTjGWTYN2wMzeGrXsdDP7o5m9m/0ec5q0knpri5mbEzNLl/rc\ntduM1y0/7TezDkm7JM2XtFfSZkmL3H1HSxupwswGJPW6e+ljwmZ2haQjkn4zMhuSmf2npIPu/tPs\nH+cUd7+tTXq7Syc4c3OTeqs2s/SPVOJzV+SM10Uo48h/kaTd7v6+ux+V9DtJC0voo+25+8uSDh63\neKGk/ux2vyp/PC1Xpbe24O5D7v5GdvuwpJGZpUt97hJ9laKM8E+T9NdR9/eqvab8dknPm9lWM+sr\nu5kxdI2aGWmfpK4ymxlDzZmbW+m4maXb5rlrZMbrovGG35fNcfdZkr4naVl2etuWvPKarZ2Ga34l\naaYq07gNSfpZmc1kM0s/IWmlux8aXSvzuRujr1KetzLCPyhpxqj707NlbcHdB7PfByQ9pcrLlHay\nf2SS1Oz3gZL7+Qd33+/un7v7MUmrVeJzl80s/YSkte7+ZLa49OdurL7Ket7KCP9mSeeY2TfNbLKk\nH0paV0IfX2Jmp2RvxMjMTpG0QO03+/A6SYuz24slPVNiL1/QLjM3V5tZWiU/d20347W7t/xH0lWq\nvOP/nqR/L6OHKn3NlPSn7OftsnuT9Lgqp4GfqfLeyBJJX5e0UdK7kp6XdHob9fZbVWZzflOVoE0t\nqbc5qpzSvylpW/ZzVdnPXaKvUp43rvADguINPyAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQf0d\ngI2CWuatsoEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x49b1710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFAFJREFUeJzt3X2MlfWVB/DvGeTVEiKik3FKFzDGhPgCccSNbynaEmtI\nsH+I5Y+FTWqnQWwWrWSJG7L8aVYL+kfTZKoTXoLAJq2RGHWDZAlbY4ijQURkV8WpQAawobECM4wz\nc/aPeTCjznPOnfu793kuPd9PQhjumd/z/O5z7+G+nN+LqCqIKJ6msjtAROVg8hMFxeQnCorJTxQU\nk58oKCY/UVBMfqKgmPxEQTH5iYK6rMiTiYiKSJGnrJg30rGpKf//Sa+tF/euiRe3jp/Sthas8w8N\nDVXdthIp16Xs65ZyblWt6MIlJb+I3AfgOQDjADyvqk85v4/LLss/pZVgQH2f5AMDA2Z80qRJuTHv\nSewde9y4cUnxr776Kjc2fvz4qtsCaY+Jd/4LFy6Ybb3H1Oubdd+s52Elce8x9+Je3y3W/RocHKz4\nOFX3QETGAfgtgJ8AmAtgmYjMrfZ4RFSslM/8CwB8rKpHVbUfwA4AS2rTLSKqt5TkbwVwbMS/j2e3\nfYOItItIl4h0cQYhUeOo+xd+qtoBoAMAmpqamP1EDSLllf8EgJkj/v397DYiugSkJP/bAK4Tkdki\nMgHAzwDsqk23iKjeqn7br6oDIvIogP/CcKmvU1U/8NpZZau+vj6zbUq5zTNhwgQzbpVQUsthqWUh\n6/he6cc7t1fy8nhlTotX6vOOPXHixNxYapmxnuNVvGNPnjw5N3b+/PmKz5P0yKrqqwBeTTkGEZWD\nw3uJgmLyEwXF5CcKislPFBSTnygoJj9RUFLkeHsRUatm7U1dtXj3w6t3W2MIALum7NXKvWm1/f39\nZty7Ltb5U+vVKY8JYD8uqescpJw7ta33fPHGrFi854s1RmFwcLDi+fx85ScKislPFBSTnygoJj9R\nUEx+oqCY/ERBFb10tzl1NmX6pzXNEUgrvXi8cphXNkpdvdfiTQf2ju1NffXumzWttre312zrTSf2\n7ptVAvWO7ZUZvb5718XKg9SydaX4yk8UFJOfKCgmP1FQTH6ioJj8REEx+YmCYvITBVVonR9IX2I7\njzct1qvrrl692oxv2LAhN5ay42olUuq6qcuG796924zfcccdZty67mvXrjXbPvvss2bcq8Vb507d\nnTh1qrM17mTKlClmW2s6sTcuYyS+8hMFxeQnCorJTxQUk58oKCY/UVBMfqKgmPxEQSUt3S0i3QC+\nBDAIYEBV26zfb2pq0pRttq24NW+8Einz1lOW1gb8Or5336y+eeeeNWuWGX/llVfM+MyZM814yhoN\na9asMeObNm0y49Z1SV0W3LuuKduLe2NSrHMPDAxgaGioojtXi0E+C1X1LzU4DhEViG/7iYJKTX4F\n8IaIvCMi7bXoEBEVI/Vt/52qekJErgawW0SOqOq+kb+Q/afQnv2ceDoiqpWkV35VPZH9fRrASwAW\njPI7Hara5n0ZSETFqjr5ReRyEZl68WcAiwAcqlXHiKi+Ut72NwN4KXsrfxmAF1X19Zr0iojqrurk\nV9WjAG4eSxsRMedJp9SEvbbe/G2P9X2FV6dPnfvtjUGwtnT2zt3a2mrGW1pazLh33VPGINx6661m\nvLOz04xbffP2efDW5U/dq8HKg5R9HsaSQyz1EQXF5CcKislPFBSTnygoJj9RUEx+oqAKXbp7aGjI\nLFt5JQ5rqqNX4kid8mv1LXXL5NQpwRZveujixYuT2nvX/fHHH8+NrVy50mx7yy23mPHZs2eb8aNH\nj+bGvH6nPibeUHbrOWOVbgG772OZos9XfqKgmPxEQTH5iYJi8hMFxeQnCorJTxQUk58oqKSlu8d8\nMhG1pjJ6tfiUqYxeLT5lm+2UKZiAP914woQJVR9/3bp1ZttHH33UjHvTiT/55BMzbo0j2Llzp9n2\n9ttvN+NHjhwx4wsXLsyNnT171mzrPV+sJegB/7rVa0m7/v7+ipfu5is/UVBMfqKgmPxEQTH5iYJi\n8hMFxeQnCorJTxRUofP5RcScq+zNkbbq4V4t3avjp2wPnlqz9dp74wgWLPjORklf8+bMe2MI+vr6\nzHhHR4cZ/+KLL3JjK1asMNvu27fPjM+ZM8eMP/TQQ7kxr9/e88UbB+A9ptbxvXEf3nz/SvGVnygo\nJj9RUEx+oqCY/ERBMfmJgmLyEwXF5CcKyq3zi0gngMUATqvqDdlt0wHsBDALQDeApar61wqOlbT2\nvlVbTVknHUibM+/V4VP75tV1n3jiidyYN+/cO/drr71mxrdu3WrGrZr1sWPHzLbeWgFXXXWVGV+1\nalVubMeOHWbbc+fOmXFPyvoQ9dzHYaRKergJwH3fum0tgD2qeh2APdm/iegS4ia/qu4DcOZbNy8B\nsDn7eTOAB2rcLyKqs2rfmzSrak/280kAzTXqDxEVJHlsv6qqiOR+6BWRdgDt2c+ppyOiGqn2lf+U\niLQAQPb36bxfVNUOVW1T1TYmP1HjqDb5dwG4OCVrBYCXa9MdIiqKm/wish3AWwCuF5HjIvJzAE8B\n+LGIfATgR9m/iegS4n7mV9VlOaF7x3qyoaEhdz1zi1Wz9mrt3j7z3hxqq27r1XS9Y8+YMcOMr1+/\n3ozffffdubH+/n6zrVcz7uzsNOPe2Azro55Xz962bZsZnz9/vhm/9tprc2MPPvig2XbLli1m3Hu+\neY+5d98t1jUfyz4cHOFHFBSTnygoJj9RUEx+oqCY/ERBMfmJgip86W6rLOaVP7yykiW1FGhNffXa\nev2+8sorzfjy5cvNuMUbVbl3714z/t5771V9bsC+7l6JtLe314xPnjzZjFtlzNmzZ5ttvcfMe66m\nxL1p1lbbsUz35Ss/UVBMfqKgmPxEQTH5iYJi8hMFxeQnCorJTxRUoXV+VTXrkF4t3qoLe8tbe7XT\nlHOntAWApUuXmvGUZcePHDlitn3kkUfMuLXFNuDXla377k039sZPpCxh7S3V7p3b27o8Zcl07/ky\nlmm7Fr7yEwXF5CcKislPFBSTnygoJj9RUEx+oqCY/ERBSa1qhpVoampSq/6ZUvf17odXK/fmvaec\n+7HHHjPjXq3d24ra6vv27dvNtitXrjTjHm+Jauu6eY/JtGnTzHhXV5cZb2lpyY319PTkxgDg5ptv\nNuPe/faeT9Zz3WtrjWnp6+vD4OBgRVtj8ZWfKCgmP1FQTH6ioJj8REEx+YmCYvITBcXkJwrKnc8v\nIp0AFgM4rao3ZLetB/ALAJ9nv/akqr5ayQlT1nG3aqve/GtvfrVXt7XWcffWEvDWiPfW7ffWgN+1\na1dubM2aNWZbb8t077p6NWnvMbVMnTrVjHvXxXquXX311XU7NuDvOZAybsR6rtZ6i+5NAO4b5faN\nqjov+1NR4hNR43CTX1X3AThTQF+IqEApn/l/JSIHRaRTRK6oWY+IqBDVJv/vAMwBMA9AD4Df5P2i\niLSLSJeIdBU5j4CIbFUlv6qeUtVBVR0C8HsAC4zf7VDVNlVt874cIqLiVJX8IjJyutRPARyqTXeI\nqCiVlPq2A/ghgBkichzAvwP4oYjMA6AAugH8so59JKI6cJNfVZeNcvML1ZxMRMy6rze/21qnfeLE\niWZbb511rx5t1fK9NeCvueYaM+7x5q0//fTTubFz586Zbb16tveYpO6XYFm4cKEZ98ZHWN8xffbZ\nZ1X16aLU8RHWdUn5bqzWdX4i+jvE5CcKislPFBSTnygoJj9RUEx+oqAK36LbmhrrldumTJmSG/NK\nL97owpSlmJubm82299xzT9XHBoD9+/eb8YMHD+bGvFKed7+taw7YU50B+7559zt1OLhVTnv99dfN\ntufPn086t1cCtcrW3mNmXbexlFb5yk8UFJOfKCgmP1FQTH6ioJj8REEx+YmCYvITBVVonR+wa/kp\nNWNvWm3qFEyL12+vbnvmjL0+6s6dO824Vdv16r7eVGhvHEBKLd5b0tzb2jyF12+vTu89n7yp1NZ1\n9x6zlOXQv3GcmhyFiC45TH6ioJj8REEx+YmCYvITBcXkJwqKyU8UVOF1fquGmTLn3pofDaTNr/bO\nfddddyUd++TJk2bcmq8P2HVf75r29/ebcW+MQmtrqxm/9957c2OrVq0y2950001m3BuDkPJ8SVlG\nHkhbRyFlq3ou3U1ELiY/UVBMfqKgmPxEQTH5iYJi8hMFxeQnCsqt84vITABbADQDUAAdqvqciEwH\nsBPALADdAJaq6l+941k1TK9GabX15uN7W3R79XBrvYC33nrLbOvVjKdPn27Gly9fbsYPHz6cG7v+\n+uvNtnPmzDHjt912mxmfNm2aGb/xxhtzY16d/tChQ2a8u7vbjK9bty43durUKbOtV2tPea4C9vPp\n7NmzZttJkyblxrzn+UiVvPIPAPi1qs4F8I8AVonIXABrAexR1esA7Mn+TUSXCDf5VbVHVd/Nfv4S\nwIcAWgEsAbA5+7XNAB6oVyeJqPbG9JlfRGYBmA9gP4BmVe3JQicx/LGAiC4RFY/tF5HvAfgDgNWq\n+reRn5FVVUVk1A9BItIOoD21o0RUWxW98ovIeAwn/jZV/WN28ykRacniLQBOj9ZWVTtUtU1V27wv\n1YioOG7yy3DGvgDgQ1XdMCK0C8CK7OcVAF6uffeIqF7EK1mIyJ0A/gfA+wAuzmN8EsOf+/8TwA8A\n/BnDpT5zDeq8jwYXectvW31N3YraK81Y8alTp5ptd+/ebcbnzp1rxnt7e8249Y7Ke3ytspF37EqO\nb/FKoGvX2gWk559/3oxb07hTS7/eFHHv+WaVplOWsL9w4QKGhoYqeovtfuZX1T8ByDtY/mRtImpo\nHOFHFBSTnygoJj9RUEx+oqCY/ERBMfmJgip06W4RMeujXs3Yint1VW8cgBe36sLedswHDhww4960\nW28cgbVdtFev9raa9sZeeEt/v/jii7mxjRs3mm0//fRTM+49ZufPn8+NeeM6UpaRB/ylva3je225\nRTcRJWHyEwXF5CcKislPFBSTnygoJj9RUEx+oqDc+fy11NTUpFad35tbbtWkvZpvyhgCwJ5j7dVd\nvb49/PDDZvyZZ54x43v37s2NLVq0yGy7adMmM/7mm2+a8a1bt5pxi7fcujcGwVsPwHpMJ06caLb1\neOMAvOeE1XevrTXmRFWhqhXN5+crP1FQTH6ioJj8REEx+YmCYvITBcXkJwqKyU8UVOF1fquW781j\ntni1dG/euVdbtWrSXr3Zqwmntvfuu8Wbl55aD7fGR3j3O3X8hPV88s6dck0raW/FvcfEMpZ1+/nK\nTxQUk58oKCY/UVBMfqKgmPxEQTH5iYJi8hMF5a7bLyIzAWwB0AxAAXSo6nMish7ALwB8nv3qk6r6\nqnc8q+7rjTmw1gJIqY0C/jgAS733DPCui3X+lGtaSXtvzr3VPvW6pMznT73mqX2zxjB4411SxyBc\nVMmmHQMAfq2q74rIVADviMjuLLZRVe2VJoioIbnJr6o9AHqyn78UkQ8BtNa7Y0RUX2P6zC8iswDM\nB7A/u+lXInJQRDpF5IqcNu0i0iUiXUUOJSYiW8XJLyLfA/AHAKtV9W8AfgdgDoB5GH5n8JvR2qlq\nh6q2qWqbN0adiIpTUfKLyHgMJ/42Vf0jAKjqKVUdVNUhAL8HsKB+3SSiWnOTX4Zfrl8A8KGqbhhx\ne8uIX/spgEO17x4R1Usl3/bfAeCfALwvIhf3mn4SwDIRmYfh8l83gF+mdsabwmmVlbypp729vWbc\n24ramtKbWuqr59RWr5TnlThTpxun8O639x1SyvLYVkka8B9zr2/WdfOOXatrXsm3/X8CMNrZ3Jo+\nETUujvAjCorJTxQUk58oKCY/UVBMfqKgmPxEQRW+dLdVL0+p63ptPSnLhnu1cK/Wnrq0t3XfvXq1\nd9286+LdN6vv3v324in3zRt74Z3bGx+Rct9S+tbX14fBwUEu3U1E+Zj8REEx+YmCYvITBcXkJwqK\nyU8UFJOfKKhC6/wi8jmAP4+4aQaAvxTWgbFp1L41ar8A9q1atezbP6jqVZX8YqHJ/52TDy/q2VZa\nBwyN2rdG7RfAvlWrrL7xbT9RUEx+oqDKTv6Oks9vadS+NWq/APatWqX0rdTP/ERUnrJf+YmoJKUk\nv4jcJyL/KyIfi8jaMvqQR0S6ReR9ETkgIl0l96VTRE6LyKERt00Xkd0i8lH296jbpJXUt/UiciK7\ndgdE5P6S+jZTRP5bRA6LyAci8i/Z7aVeO6NfpVy3wt/2i8g4AP8H4McAjgN4G8AyVT1caEdyiEg3\ngDZVLb0mLCJ3AzgLYIuq3pDd9h8AzqjqU9l/nFeo6r82SN/WAzhb9s7N2YYyLSN3lgbwAIB/RonX\nzujXUpRw3cp45V8A4GNVPaqq/QB2AFhSQj8anqruA3DmWzcvAbA5+3kzhp88hcvpW0NQ1R5VfTf7\n+UsAF3eWLvXaGf0qRRnJ3wrg2Ih/H0djbfmtAN4QkXdEpL3szoyiOds2HQBOAmguszOjcHduLtK3\ndpZumGtXzY7XtcYv/L7rTlWdB+AnAFZlb28bkg5/Zmukck1FOzcXZZSdpb9W5rWrdsfrWisj+U8A\nmDni39/PbmsIqnoi+/s0gJfQeLsPn7q4SWr29+mS+/O1Rtq5ebSdpdEA166RdrwuI/nfBnCdiMwW\nkQkAfgZgVwn9+A4RuTz7IgYicjmARWi83Yd3AViR/bwCwMsl9uUbGmXn5rydpVHytWu4Ha9VtfA/\nAO7H8Df+nwD4tzL6kNOvOQDey/58UHbfAGzH8NvArzD83cjPAVwJYA+AjwC8AWB6A/VtK4D3ARzE\ncKK1lNS3OzH8lv4ggAPZn/vLvnZGv0q5bhzhRxQUv/AjCorJTxQUk58oKCY/UVBMfqKgmPxEQTH5\niYJi8hMF9f9MDxYkSWlskgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21ff198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Augmenting data...\"\n",
    "_X = [X_3K.copy()]\n",
    "_y = [y_3K.copy()]\n",
    "for i in xrange(7):\n",
    "    _X.append(X_3K.copy() + np.random.normal(scale=3, size=X_3K.shape))\n",
    "    _y.append(y_3K.copy())\n",
    "X_aug = np.vstack(_X)\n",
    "y_aug = np.hstack(_y)\n",
    "\n",
    "plt.imshow(X_3K[-1].reshape(28,28), cmap=\"gray\")\n",
    "plt.show()\n",
    "plt.imshow(X_aug[-1].reshape(28,28), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting KNN (with augmentation)\n",
      "train accuracy: 1.0\n",
      "test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_aug, y_aug, test_size=0.33, random_state=43)\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "print \"fitting KNN (with augmentation)\"\n",
    "model.fit(X_train, y_train)\n",
    "print \"train accuracy:\", accuracy_score(y_train, model.predict(X_train))\n",
    "print \"test accuracy:\", accuracy_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurra! Po dodaniu stosunkowo prostej augmentacji danych nasz model nauczył się perfekcyjnie generalizować na zbiór testowy! I nie musieliśmy tu stosować żadnej skomplikowanej sieci neuronowej, wystarczył KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie 5 (4 pkt)\n",
    "\n",
    "Wytłumaczyć, na czym polega błąd w powyższym rozumowaniu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>Błąd polega na tym, że augmentację robimy przed podziałem zbioru na trenujący i testujący, przez co nasz model uczy się prawie identycznych przykładów, które są w zbiorze testującym (bo jeśli dokładamy 7 dodatkowych, zaszumionych przykładów, co daje w sumie 8 razem z oryginalnym, to któreś z nich trafią do zbioru trenującego, a któreś trafią do zbioru testującego). Nie różni się to zatem prawie niczym od testowania modelu na zbiorze trenującym, co, jak wiemy, jest złą miarą poprawności modelu. Poprawnie możemy to zrobić dzieląc zbiór najpierw na trnujący i testujący i później na onu tych zbiorach można zrobić augmentację. Jak poniżej: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting data...\n",
      "fitting KNN (with augmentation)\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.922853535354\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_3K, y_3K, test_size=0.33, random_state=43)\n",
    "\n",
    "print \"Augmenting data...\"\n",
    "_X_train = [X_train.copy()]\n",
    "_y_train = [y_train.copy()]\n",
    "_X_test = [X_test.copy()]\n",
    "_y_test = [y_test.copy()]\n",
    "for i in xrange(7):\n",
    "    _X_train.append(X_train.copy() + np.random.normal(scale=3, size=X_train.shape))\n",
    "    _X_test.append(X_test.copy() + np.random.normal(scale=3, size=X_test.shape))\n",
    "    _y_train.append(y_train.copy())\n",
    "    _y_test.append(y_test.copy())\n",
    "X_train_aug = np.vstack(_X_train)\n",
    "X_test_aug = np.vstack(_X_test)\n",
    "y_train_aug = np.hstack(_y_train)\n",
    "y_test_aug = np.hstack(_y_test)\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "print \"fitting KNN (with augmentation)\"\n",
    "model.fit(X_train_aug, y_train_aug)\n",
    "print \"train accuracy:\", accuracy_score(y_train_aug, model.predict(X_train_aug))\n",
    "print \"test accuracy:\", accuracy_score(y_test_aug, model.predict(X_test_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
