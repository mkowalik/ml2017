{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projekt ML - Michał Kowalik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Część I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "ERROR (theano.gpuarray): Could not initialize pygpu, support disabled\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda2\\lib\\site-packages\\theano\\gpuarray\\__init__.py\", line 179, in <module>\n",
      "    use(config.device)\n",
      "  File \"C:\\ProgramData\\Anaconda2\\lib\\site-packages\\theano\\gpuarray\\__init__.py\", line 166, in use\n",
      "    init_dev(device, preallocate=preallocate)\n",
      "  File \"C:\\ProgramData\\Anaconda2\\lib\\site-packages\\theano\\gpuarray\\__init__.py\", line 65, in init_dev\n",
      "    sched=config.gpuarray.sched)\n",
      "  File \"pygpu\\gpuarray.pyx\", line 614, in pygpu.gpuarray.init (pygpu/gpuarray.c:9415)\n",
      "  File \"pygpu\\gpuarray.pyx\", line 566, in pygpu.gpuarray.pygpu_init (pygpu/gpuarray.c:9106)\n",
      "  File \"pygpu\\gpuarray.pyx\", line 1021, in pygpu.gpuarray.GpuContext.__cinit__ (pygpu/gpuarray.c:13468)\n",
      "GpuArrayException: Error loading library: 0\n"
     ]
    }
   ],
   "source": [
    "# Baseline dla zbioru CIFAR-10 - regresja logistyczna w wersji multiclass\n",
    "# Przy submitowaniu predykcji proszę używać funkcji save_labels\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "os.environ['THEANO_FLAGS'] = \"device=cuda0\"  \n",
    "\n",
    "import keras\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold\n",
    "\n",
    "import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input, UpSampling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ładujemy dane, przy okazji przekształcając je do postaci lubianej przez keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_labels(arr, filename):\n",
    "    pd_array = pd.DataFrame(arr)\n",
    "    pd_array.index.names = [\"Id\"]\n",
    "    pd_array.columns = [\"Prediction\"]\n",
    "    pd_array.to_csv(filename)\n",
    "\n",
    "def load_labels(filename):\n",
    "    return pd.read_csv(filename, index_col=0).values.ravel()\n",
    "\n",
    "X_train = np.load(\"X_train.npy\")\n",
    "y_train = load_labels(\"y_train.csv\")\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "\n",
    "X_train_small = np.load(\"X_train_small.npy\")\n",
    "y_train_small = load_labels(\"y_train_small.csv\")\n",
    "\n",
    "y_train_one_hot = keras.utils.to_categorical(y_train)\n",
    "y_train_small_one_hot = keras.utils.to_categorical(y_train_small)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przekształcamy na float i skalujemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000L, 3072L)\n",
      "(10000L, 3072L)\n",
      "(5000L, 3072L)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "X_train_small = X_train_small.astype('float32') / 255.0\n",
    "\n",
    "print (X_train.shape)\n",
    "print (X_test.shape)\n",
    "print (X_train_small.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "0\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print (y_train_small.max())\n",
    "print (y_train_small.min())\n",
    "classes_number = y_train_small.max() - y_train_small.min() + 1\n",
    "print (classes_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie dzielimy na dane trenujące i testujące. Moglibyśmy użyć cross-entropy, ale bardzo by to wydłużyło cały proces doboru parametrów. Chcemy tylko mniej-więcej wybrać parametry, a następnie i tak je będziemy testować na całym zbiorze trenującym już przy użyciu cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr_s, X_te_s, y_tr_s, y_te_s = train_test_split(X_train_small, y_train_small_one_hot, test_size=0.25)\n",
    "X_tr, y_tr = X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spróbujemy wybrać najlepsze parametry dla sieci neuronowej. Jest to:\n",
    "\n",
    "<b>learning rate</b> - parametr stanowiący o szybkości uczenia się sieci - współczynnik przy gradiencie wag w algorytmie Gradient Descent. Gdy jest zbyt mały - sieć uczy się bardzo wolno, zbyt duży - przeskakuje przez minimum, przez co oddala się od optymalnego rozwiązania. <br>\n",
    "<b>activation_first</b> - funkcja aktywacji pierwszej warstwy <br>\n",
    "<b>actication_second</b>- funkcja aktywacji drugiej warstwy <br>\n",
    "<b>hidden_size</b> - liczba neuronów w warstwie ukrytej <br>\n",
    "Najważniejsze z nich, to <b>hidden_size</b> i <b>learning_rate</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing lr: 0.01 activations: relu , softmax hidden layer size: 10\n",
      "Score:  0.2928\n",
      "Testing lr: 0.01 activations: relu , softmax hidden layer size: 50\n",
      "Score:  0.3472\n",
      "Testing lr: 0.01 activations: relu , softmax hidden layer size: 100\n",
      "Score:  0.38\n",
      "Testing lr: 0.01 activations: relu , softmax hidden layer size: 250\n",
      "Score:  0.3848\n",
      "Testing lr: 0.01 activations: relu , softmax hidden layer size: 500\n",
      "Score:  0.3848\n",
      "Testing lr: 0.01 activations: relu , softmax hidden layer size: 1000\n",
      "Score:  0.3992\n",
      "Testing lr: 0.01 activations: relu , sigmoid hidden layer size: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock C:\\Users\\Kowalik\\AppData\\Local\\Theano\\compiledir_Windows-10-10.0.15063-Intel64_Family_6_Model_69_Stepping_1_GenuineIntel-2.7.13-64\\lock_dir\\lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock C:\\Users\\Kowalik\\AppData\\Local\\Theano\\compiledir_Windows-10-10.0.15063-Intel64_Family_6_Model_69_Stepping_1_GenuineIntel-2.7.13-64\\lock_dir\\lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.2592\n",
      "Testing lr: 0.01 activations: relu , sigmoid hidden layer size: 50\n",
      "Score:  0.3216\n",
      "Testing lr: 0.01 activations: relu , sigmoid hidden layer size: 100\n",
      "Score:  0.32\n",
      "Testing lr: 0.01 activations: relu , sigmoid hidden layer size: 250\n",
      "Score:  0.348\n",
      "Testing lr: 0.01 activations: relu , sigmoid hidden layer size: 500\n",
      "Score:  0.3496\n",
      "Testing lr: 0.01 activations: relu , sigmoid hidden layer size: 1000\n",
      "Score:  0.3952\n",
      "Testing lr: 0.01 activations: softmax , softmax hidden layer size: 10\n",
      "Score:  0.216\n",
      "Testing lr: 0.01 activations: softmax , softmax hidden layer size: 50\n",
      "Score:  0.16\n",
      "Testing lr: 0.01 activations: softmax , softmax hidden layer size: 100\n",
      "Score:  0.1552\n",
      "Testing lr: 0.01 activations: softmax , softmax hidden layer size: 250\n",
      "Score:  0.0896\n",
      "Testing lr: 0.01 activations: softmax , softmax hidden layer size: 500\n",
      "Score:  0.0944\n",
      "Testing lr: 0.01 activations: softmax , softmax hidden layer size: 1000\n",
      "Score:  0.0952\n",
      "Testing lr: 0.01 activations: softmax , sigmoid hidden layer size: 10\n",
      "Score:  0.1856\n",
      "Testing lr: 0.01 activations: softmax , sigmoid hidden layer size: 50\n",
      "Score:  0.14\n",
      "Testing lr: 0.01 activations: softmax , sigmoid hidden layer size: 100\n",
      "Score:  0.1424\n",
      "Testing lr: 0.01 activations: softmax , sigmoid hidden layer size: 250\n",
      "Score:  0.1024\n",
      "Testing lr: 0.01 activations: softmax , sigmoid hidden layer size: 500\n",
      "Score:  0.0808\n",
      "Testing lr: 0.01 activations: softmax , sigmoid hidden layer size: 1000\n",
      "Score:  0.116\n",
      "Testing lr: 0.05 activations: relu , softmax hidden layer size: 10\n",
      "Score:  0.3552\n",
      "Testing lr: 0.05 activations: relu , softmax hidden layer size: 50\n",
      "Score:  0.3768\n",
      "Testing lr: 0.05 activations: relu , softmax hidden layer size: 100\n",
      "Score:  0.4248\n",
      "Testing lr: 0.05 activations: relu , softmax hidden layer size: 250\n",
      "Score:  0.4032\n",
      "Testing lr: 0.05 activations: relu , softmax hidden layer size: 500\n",
      "Score:  0.416\n",
      "Testing lr: 0.05 activations: relu , softmax hidden layer size: 1000\n",
      "Score:  0.428\n",
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 10\n",
      "Score:  0.2144\n",
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 50\n",
      "Score:  0.3736\n",
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 100\n",
      "Score:  0.3704\n",
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 250\n",
      "Score:  0.4136\n",
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 500\n",
      "Score:  0.4112\n",
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 1000\n",
      "Score:  0.4224\n",
      "Testing lr: 0.05 activations: softmax , softmax hidden layer size: 10\n",
      "Score:  0.2712\n",
      "Testing lr: 0.05 activations: softmax , softmax hidden layer size: 50\n",
      "Score:  0.256\n",
      "Testing lr: 0.05 activations: softmax , softmax hidden layer size: 100\n",
      "Score:  0.2144\n",
      "Testing lr: 0.05 activations: softmax , softmax hidden layer size: 250\n",
      "Score:  0.204\n",
      "Testing lr: 0.05 activations: softmax , softmax hidden layer size: 500\n",
      "Score:  0.1208\n",
      "Testing lr: 0.05 activations: softmax , softmax hidden layer size: 1000\n",
      "Score:  0.1008\n",
      "Testing lr: 0.05 activations: softmax , sigmoid hidden layer size: 10\n",
      "Score:  0.2136\n",
      "Testing lr: 0.05 activations: softmax , sigmoid hidden layer size: 50\n",
      "Score:  0.2328\n",
      "Testing lr: 0.05 activations: softmax , sigmoid hidden layer size: 100\n",
      "Score:  0.2008\n",
      "Testing lr: 0.05 activations: softmax , sigmoid hidden layer size: 250\n",
      "Score:  0.14\n",
      "Testing lr: 0.05 activations: softmax , sigmoid hidden layer size: 500\n",
      "Score:  0.0968\n",
      "Testing lr: 0.05 activations: softmax , sigmoid hidden layer size: 1000\n",
      "Score:  0.0864\n",
      "Testing lr: 0.1 activations: relu , softmax hidden layer size: 10\n",
      "Score:  0.3368\n",
      "Testing lr: 0.1 activations: relu , softmax hidden layer size: 50\n",
      "Score:  0.3752\n",
      "Testing lr: 0.1 activations: relu , softmax hidden layer size: 100\n",
      "Score:  0.3752\n",
      "Testing lr: 0.1 activations: relu , softmax hidden layer size: 250\n",
      "Score:  0.4\n",
      "Testing lr: 0.1 activations: relu , softmax hidden layer size: 500\n",
      "Score:  0.3944\n",
      "Testing lr: 0.1 activations: relu , softmax hidden layer size: 1000\n",
      "Score:  0.4168\n",
      "Testing lr: 0.1 activations: relu , sigmoid hidden layer size: 10\n",
      "Score:  0.2968\n",
      "Testing lr: 0.1 activations: relu , sigmoid hidden layer size: 50\n",
      "Score:  0.36\n",
      "Testing lr: 0.1 activations: relu , sigmoid hidden layer size: 100\n",
      "Score:  0.372\n",
      "Testing lr: 0.1 activations: relu , sigmoid hidden layer size: 250\n",
      "Score:  0.416\n",
      "Testing lr: 0.1 activations: relu , sigmoid hidden layer size: 500\n",
      "Score:  0.3816\n",
      "Testing lr: 0.1 activations: relu , sigmoid hidden layer size: 1000\n",
      "Score:  0.3832\n",
      "Testing lr: 0.1 activations: softmax , softmax hidden layer size: 10\n",
      "Score:  0.2624\n",
      "Testing lr: 0.1 activations: softmax , softmax hidden layer size: 50\n",
      "Score:  0.2976\n",
      "Testing lr: 0.1 activations: softmax , softmax hidden layer size: 100\n",
      "Score:  0.2152\n",
      "Testing lr: 0.1 activations: softmax , softmax hidden layer size: 250\n",
      "Score:  0.2032\n",
      "Testing lr: 0.1 activations: softmax , softmax hidden layer size: 500\n",
      "Score:  0.1856\n",
      "Testing lr: 0.1 activations: softmax , softmax hidden layer size: 1000\n",
      "Score:  0.1112\n",
      "Testing lr: 0.1 activations: softmax , sigmoid hidden layer size: 10\n",
      "Score:  0.1672\n",
      "Testing lr: 0.1 activations: softmax , sigmoid hidden layer size: 50\n",
      "Score:  0.2048\n",
      "Testing lr: 0.1 activations: softmax , sigmoid hidden layer size: 100\n",
      "Score:  0.2144\n",
      "Testing lr: 0.1 activations: softmax , sigmoid hidden layer size: 250\n",
      "Score:  0.1784\n",
      "Testing lr: 0.1 activations: softmax , sigmoid hidden layer size: 500\n",
      "Score:  0.1408\n",
      "Testing lr: 0.1 activations: softmax , sigmoid hidden layer size: 1000\n",
      "Score:  0.1064\n",
      "Testing lr: 0.2 activations: relu , softmax hidden layer size: 10\n",
      "Score:  0.3576\n",
      "Testing lr: 0.2 activations: relu , softmax hidden layer size: 50\n",
      "Score:  0.3496\n",
      "Testing lr: 0.2 activations: relu , softmax hidden layer size: 100\n",
      "Score:  0.3808\n",
      "Testing lr: 0.2 activations: relu , softmax hidden layer size: 250\n",
      "Score:  0.3456\n",
      "Testing lr: 0.2 activations: relu , softmax hidden layer size: 500\n",
      "Score:  0.3416\n",
      "Testing lr: 0.2 activations: relu , softmax hidden layer size: 1000\n",
      "Score:  0.3888\n",
      "Testing lr: 0.2 activations: relu , sigmoid hidden layer size: 10\n",
      "Score:  0.3032\n",
      "Testing lr: 0.2 activations: relu , sigmoid hidden layer size: 50\n",
      "Score:  0.364\n",
      "Testing lr: 0.2 activations: relu , sigmoid hidden layer size: 100\n",
      "Score:  0.3944\n",
      "Testing lr: 0.2 activations: relu , sigmoid hidden layer size: 250\n",
      "Score:  0.3568\n",
      "Testing lr: 0.2 activations: relu , sigmoid hidden layer size: 500\n",
      "Score:  0.3712\n",
      "Testing lr: 0.2 activations: relu , sigmoid hidden layer size: 1000\n",
      "Score:  0.3384\n",
      "Testing lr: 0.2 activations: softmax , softmax hidden layer size: 10\n",
      "Score:  0.316\n",
      "Testing lr: 0.2 activations: softmax , softmax hidden layer size: 50\n",
      "Score:  0.2904\n",
      "Testing lr: 0.2 activations: softmax , softmax hidden layer size: 100\n",
      "Score:  0.2376\n",
      "Testing lr: 0.2 activations: softmax , softmax hidden layer size: 250\n",
      "Score:  0.2528\n",
      "Testing lr: 0.2 activations: softmax , softmax hidden layer size: 500\n",
      "Score:  0.252\n",
      "Testing lr: 0.2 activations: softmax , softmax hidden layer size: 1000\n",
      "Score:  0.2072\n",
      "Testing lr: 0.2 activations: softmax , sigmoid hidden layer size: 10\n",
      "Score:  0.2808\n",
      "Testing lr: 0.2 activations: softmax , sigmoid hidden layer size: 50\n",
      "Score:  0.2392\n",
      "Testing lr: 0.2 activations: softmax , sigmoid hidden layer size: 100\n",
      "Score:  0.2672\n",
      "Testing lr: 0.2 activations: softmax , sigmoid hidden layer size: 250\n",
      "Score:  0.1992\n",
      "Testing lr: 0.2 activations: softmax , sigmoid hidden layer size: 500\n",
      "Score:  0.172\n",
      "Testing lr: 0.2 activations: softmax , sigmoid hidden layer size: 1000\n",
      "Score:  0.1376\n"
     ]
    }
   ],
   "source": [
    "# Najpierw zobaczymy jak sobie radzi na prostej sieci 3-warstwowej na malych danych\n",
    "\n",
    "for lr in [0.01, 0.05, 0.1, 0.2]:\n",
    "    for activation_first in ['relu', 'softmax']:\n",
    "        for activation_second in ['softmax', 'sigmoid']:\n",
    "            for hidden_size in [10, 50, 100, 250, 500, 1000]:\n",
    "\n",
    "                batch = 1000\n",
    "\n",
    "                model = Sequential()\n",
    "                model.add(Dense(hidden_size, input_shape=(X_train_small.shape[1],)))\n",
    "                model.add(Activation(activation_first))\n",
    "                model.add(Dense(classes_number, input_shape=(hidden_size, )))\n",
    "                model.add(Activation(activation_second))\n",
    "\n",
    "                model.compile(optimizer=SGD(lr=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "                print \"Testing lr:\", lr, \"activations:\", activation_first, \",\", activation_second, \\\n",
    "                    \"hidden layer size:\", hidden_size\n",
    "\n",
    "                model.fit(X_tr_s, y_tr_s, epochs=50, batch_size=batch, verbose=0)\n",
    "\n",
    "                y_pred = model.predict(X_te_s, batch_size=batch)\n",
    "\n",
    "                print \"Score: \", accuracy_score(y_te_s.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, ze siec osiaga najlepsze wyniki dla lr=0.05, funkcji aktywacji relu, sigmoid i wiekosci warsty ukrytej na poziomie 500-1000. Zatem sprobujmy przeprowadzić cross-validację sieci na dla calych danych Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 500\n",
      "Running Fold 1 / 3\n",
      "Score:  0.50575884823\n",
      "Running Fold 2 / 3\n",
      "Score:  0.456328734253\n",
      "Running Fold 3 / 3\n",
      "Score:  0.467647058824\n",
      "Score mean:  0.476578213769\n",
      "\n",
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 1000\n",
      "Running Fold 1 / 3\n",
      "Score:  0.472765446911\n",
      "Running Fold 2 / 3\n",
      "Score:  0.453929214157\n",
      "Running Fold 3 / 3\n",
      "Score:  0.476770708283\n",
      "Score mean:  0.467821789784\n",
      "\n",
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 1500\n",
      "Running Fold 1 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock C:\\Users\\Kowalik\\AppData\\Local\\Theano\\compiledir_Windows-10-10.0.15063-Intel64_Family_6_Model_69_Stepping_1_GenuineIntel-2.7.13-64\\lock_dir\\lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock C:\\Users\\Kowalik\\AppData\\Local\\Theano\\compiledir_Windows-10-10.0.15063-Intel64_Family_6_Model_69_Stepping_1_GenuineIntel-2.7.13-64\\lock_dir\\lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.511937612478\n",
      "Running Fold 2 / 3\n",
      "Score:  0.495440911818\n",
      "Running Fold 3 / 3\n",
      "Score:  0.502040816327\n",
      "Score mean:  0.503139780207\n",
      "\n",
      "Testing lr: 0.05 activations: relu , sigmoid hidden layer size: 2000\n",
      "Running Fold 1 / 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock C:\\Users\\Kowalik\\AppData\\Local\\Theano\\compiledir_Windows-10-10.0.15063-Intel64_Family_6_Model_69_Stepping_1_GenuineIntel-2.7.13-64\\lock_dir\\lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock C:\\Users\\Kowalik\\AppData\\Local\\Theano\\compiledir_Windows-10-10.0.15063-Intel64_Family_6_Model_69_Stepping_1_GenuineIntel-2.7.13-64\\lock_dir\\lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.490161967606\n",
      "Running Fold 2 / 3\n",
      "Score:  0.478764247151\n",
      "Running Fold 3 / 3\n",
      "Score:  0.506842737095\n",
      "Score mean:  0.491922983951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = 0.05\n",
    "activation_first = 'relu'\n",
    "activation_second = 'sigmoid'\n",
    "hidden_sizes = [500, 1000, 1500, 2000]\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    \n",
    "    n_folds = 3\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    \n",
    "    print \"Testing lr:\", lr, \"activations:\", activation_first, \",\", activation_second, \\\n",
    "        \"hidden layer size:\", hidden_size\n",
    "        \n",
    "    scores = []\n",
    "\n",
    "    for i, (train, test) in enumerate(skf.split(X_tr, y_train)):\n",
    "        print \"Running Fold\", i+1, \"/\", n_folds\n",
    "\n",
    "        batch = 1000\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(hidden_size, input_shape=(X_tr.shape[1],)))\n",
    "        model.add(Activation(activation_first))\n",
    "        model.add(Dense(classes_number, input_shape=(hidden_size, )))\n",
    "        model.add(Activation(activation_second))\n",
    "\n",
    "        model.compile(optimizer=SGD(lr=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "        model.fit(X_tr[train], y_train_one_hot[train], epochs=50, batch_size=batch, verbose=0)\n",
    "        \n",
    "        y_pred = model.predict(X_tr[test], batch_size=batch)\n",
    "        \n",
    "        score = accuracy_score(y_train_one_hot[test].argmax(axis=1), y_pred.argmax(axis=1))\n",
    "        print \"Score: \", score\n",
    "        scores.append(score)\n",
    "\n",
    "    print \"Score mean: \", np.mean(scores)\n",
    "    print \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy zatem, że na prostej sieci neuronowej z jedną warstwą ukrytą, na cross-entropy estymator z 3-ma foldami na zbiorze Train osiąga wynik na poziomie 50% accuracy, przy learning rate 0.05, funkcji aktywacji pierwszej warstwy: relu i funkcji aktywacji drugiej warstwy: sigmoid i rozmiarze ukrytej warstwy na poziomie 1500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moglibyśmy jeszcze dobrać optymalną liczbę epok uczenia, jednak zapewne wynik uda nam się podbić nieznacznie, zatem spróbujemy użyć innej metody, jaką jest <b>prosta sieć konwolucyjna</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przygotujmy dane. Spakujmy w odpowiedni sposób wartości poszczególnych kolorów dla danych pikseli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pack_color_image(tab):\n",
    "    tab_red = tab[:,:1024]\n",
    "    tab_green = tab[:,1024:2048]\n",
    "    tab_blue = tab[:,2048:3072]\n",
    "\n",
    "    ret = np.dstack((tab_red, tab_green, tab_blue))\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750L, 32L, 32L, 3L)\n",
      "(1250L, 32L, 32L, 3L)\n",
      "(3750L, 10L)\n",
      "(1250L, 10L)\n",
      "10\n",
      "(50000L, 32L, 32L, 3L)\n"
     ]
    }
   ],
   "source": [
    "X_tr_s_reshaped = pack_color_image(X_tr_s).reshape(-1, 32, 32, 3)\n",
    "X_te_s_reshaped = pack_color_image(X_te_s).reshape(-1, 32, 32, 3)\n",
    "X_train_small_reshaped = pack_color_image(X_train_small).reshape(-1, 32, 32, 3)\n",
    "print (X_tr_s_reshaped.shape)\n",
    "print (X_te_s_reshaped.shape)\n",
    "\n",
    "print (y_tr_s.shape)\n",
    "print (y_te_s.shape)\n",
    "\n",
    "num_classes = y_te_s.shape[1]\n",
    "print (num_classes)\n",
    "\n",
    "X_tr_reshaped = pack_color_image(X_tr).reshape(-1, 32, 32, 3)\n",
    "print (X_tr_reshaped.shape)\n",
    "\n",
    "X_test_reshaped = pack_color_image(X_test).reshape(-1, 32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x330df438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = X_tr_s_reshaped[2]\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na podzielonych danych na trenujące i testujące próbujemy dobrać najlepsze parametry dla tej sieci. Z powodu długich obliczeń, robimy to na danych ze zebioru small, a później zrobimy cross-validację modelu na pełnych danych. Testowane parametry:\n",
    "\n",
    "<b>dense_size</b> - rozmiar warstwy ukrytej pomiędzy częścią konwolucyjną a sekwencyjną<br>\n",
    "<b>dropout</b> - wartość parametru dropout dla dwóch warstw Dropout w modelu sieci<br>\n",
    "<b>lrate</b> - learning rate dla algorytmu SGD<br>\n",
    "<b>kernel_size</b> - rozmiar kernali dla warstw konwolucyjnych sieci<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# models = []\n",
    "scores = []\n",
    "fit_histories = []\n",
    "\n",
    "for dense_size in [512, 1000]:\n",
    "    for dropout in [0.2, 0.5]:\n",
    "        for lrate in [0.1, 0.01, 0.001]:\n",
    "            for kernel_size in [(3,3), (5,5), (8,8)]:\n",
    "\n",
    "                print \"Training network on: \"\n",
    "                print \"kernel size:\", kernel_size\n",
    "                print \"lrate:      \", lrate\n",
    "                print \"dropout:    \", dropout\n",
    "                print \"dense size: \", dense_size\n",
    "\n",
    "                model = Sequential()\n",
    "\n",
    "                model.add(Conv2D(32, kernel_size, input_shape=(32, 32, 3), padding='same', activation='relu'))\n",
    "                model.add(Dropout(dropout))\n",
    "                model.add(Conv2D(32, kernel_size, activation='relu', padding='same'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "                model.add(Flatten())\n",
    "                model.add(Dense(dense_size, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "                model.add(Dropout(dropout))\n",
    "                model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "                # Compile model\n",
    "                epochs = 25\n",
    "                decay = lrate/epochs\n",
    "                momentum = 0.9\n",
    "                sgd = SGD(lr=lrate, momentum=momentum, decay=decay)\n",
    "                model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "                fit_history = model.fit(X_tr_s_reshaped, y_tr_s, validation_data=(X_te_s_reshaped, y_te_s), epochs=epochs, batch_size=32, verbose=2)\n",
    "                score = model.evaluate(X_te_s_reshaped, y_te_s, verbose=0)[1]*100\n",
    "\n",
    "#                 models.append(model)\n",
    "                scores.append(score)\n",
    "                fit_histories.append(fit_history)\n",
    "                print(\"Accuracy: %.2f%%\" % score)\n",
    "                print\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki policzone na komputerze z GPU odostępnionym dzięki uprzejmości kolegi, Michała Goldy. Podsumowanie poniżej. Pełny log w załączonym pliku 'out.out'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>(50000, 3072)\n",
    "(10000, 3072)\n",
    "(5000, 3072)\n",
    "9\n",
    "0\n",
    "10\n",
    "(3750, 32, 32, 3)\n",
    "(1250, 32, 32, 3)\n",
    "(3750, 10)\n",
    "(1250, 10)\n",
    "10\n",
    "Training network on: \n",
    "kernel size: (3, 3)\n",
    "lrate:       0.0001\n",
    "dropout:     0.2\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 42.08%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (5, 5)\n",
    "lrate:       0.0001\n",
    "dropout:     0.2\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 44.40%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (8, 8)\n",
    "lrate:       0.0001\n",
    "dropout:     0.2\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 42.96%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (3, 3)\n",
    "lrate:       0.001\n",
    "dropout:     0.2\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 44.08%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (5, 5)\n",
    "lrate:       0.001\n",
    "dropout:     0.2\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 43.92%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (8, 8)\n",
    "lrate:       0.001\n",
    "dropout:     0.2\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 40.48%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (3, 3)\n",
    "lrate:       0.01\n",
    "dropout:     0.2\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 27.04%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (5, 5)\n",
    "lrate:       0.01\n",
    "dropout:     0.2\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 9.60%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (8, 8)\n",
    "lrate:       0.01\n",
    "dropout:     0.2\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 23.12%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (3, 3)\n",
    "lrate:       0.1\n",
    "dropout:     0.2\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 9.36%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (5, 5)\n",
    "lrate:       0.1\n",
    "dropout:     0.2\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 9.60%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (8, 8)\n",
    "lrate:       0.1\n",
    "dropout:     0.2\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 9.68%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (3, 3)\n",
    "lrate:       0.0001\n",
    "dropout:     0.5\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 41.68%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (5, 5)\n",
    "lrate:       0.0001\n",
    "dropout:     0.5\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 43.68%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (8, 8)\n",
    "lrate:       0.0001\n",
    "dropout:     0.5\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 44.40%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (3, 3)\n",
    "lrate:       0.001\n",
    "dropout:     0.5\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 39.92%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (5, 5)\n",
    "lrate:       0.001\n",
    "dropout:     0.5\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 38.24%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (8, 8)\n",
    "lrate:       0.001\n",
    "dropout:     0.5\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 31.60%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (3, 3)\n",
    "lrate:       0.01\n",
    "dropout:     0.5\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 9.44%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (5, 5)\n",
    "lrate:       0.01\n",
    "dropout:     0.5\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 9.76%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (8, 8)\n",
    "lrate:       0.01\n",
    "dropout:     0.5\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 9.68%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (3, 3)\n",
    "lrate:       0.1\n",
    "dropout:     0.5\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 9.68%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (5, 5)\n",
    "lrate:       0.1\n",
    "dropout:     0.5\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 10.08%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (8, 8)\n",
    "lrate:       0.1\n",
    "dropout:     0.5\n",
    "dense size:  512\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 9.68%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (3, 3)\n",
    "lrate:       0.0001\n",
    "dropout:     0.2\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 41.20%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (5, 5)\n",
    "lrate:       0.0001\n",
    "dropout:     0.2\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 44.48%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (8, 8)\n",
    "lrate:       0.0001\n",
    "dropout:     0.2\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 45.52%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (3, 3)\n",
    "lrate:       0.001\n",
    "dropout:     0.2\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 47.68%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (5, 5)\n",
    "lrate:       0.001\n",
    "dropout:     0.2\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 44.72%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (8, 8)\n",
    "lrate:       0.001\n",
    "dropout:     0.2\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 38.72%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (3, 3)\n",
    "lrate:       0.01\n",
    "dropout:     0.2\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 9.60%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (5, 5)\n",
    "lrate:       0.01\n",
    "dropout:     0.2\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 9.44%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (8, 8)\n",
    "lrate:       0.01\n",
    "dropout:     0.2\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 21.84%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (3, 3)\n",
    "lrate:       0.1\n",
    "dropout:     0.2\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 10.08%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (5, 5)\n",
    "lrate:       0.1\n",
    "dropout:     0.2\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 9.76%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (8, 8)\n",
    "lrate:       0.1\n",
    "dropout:     0.2\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 9.76%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (3, 3)\n",
    "lrate:       0.0001\n",
    "dropout:     0.5\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 42.24%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (5, 5)\n",
    "lrate:       0.0001\n",
    "dropout:     0.5\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 45.36%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (8, 8)\n",
    "lrate:       0.0001\n",
    "dropout:     0.5\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 43.52%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (3, 3)\n",
    "lrate:       0.001\n",
    "dropout:     0.5\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 45.36%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (5, 5)\n",
    "lrate:       0.001\n",
    "dropout:     0.5\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 38.64%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (8, 8)\n",
    "lrate:       0.001\n",
    "dropout:     0.5\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 33.92%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (3, 3)\n",
    "lrate:       0.01\n",
    "dropout:     0.5\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 9.60%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (5, 5)\n",
    "lrate:       0.01\n",
    "dropout:     0.5\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 12.56%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (8, 8)\n",
    "lrate:       0.01\n",
    "dropout:     0.5\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 10.56%\n",
    "\n",
    "Training network on: \n",
    "kernel size: (3, 3)\n",
    "lrate:       0.1\n",
    "dropout:     0.5\n",
    "dense size:  1024\n",
    "Train on 3750 samples, validate on 1250 samples\n",
    "Accuracy: 10.24%\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Widzimy, że na danych testowych najlepsze wyniki uzyskiwane są dla parametrów: \n",
    "\n",
    "dense_size = 1024, kernel_size=(3,3), dropout=0.2, lrate 0.0001 i 0.001.\n",
    "\n",
    "Zatem przeprowadzimy cross-validację modelu na danych (X_train, y_train) i tych parametrów. <br>\n",
    "Największe znaczenie miało tutaj lrate, które zbyt duże nie pozwalało uczyć modelu (wyniki wyraźnie odbiegały)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_size = 1024\n",
    "dropout = 0.2\n",
    "kernel_size = (3,3)\n",
    "\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "\n",
    "for lrate in [0.0005, 0.001]:\n",
    "    \n",
    "    n_folds = 2\n",
    "    skf = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "\n",
    "    print (\"Training network on: \")\n",
    "    print (\"kernel size:\", kernel_size)\n",
    "    print (\"lrate:      \", lrate)\n",
    "    print (\"dropout:    \", dropout)\n",
    "    print (\"dense size: \", dense_size)\n",
    "        \n",
    "    scores = []\n",
    "\n",
    "    for i, (train, test) in enumerate(skf.split(X_tr, y_train)):\n",
    "        print\n",
    "        print (\"Running Fold\", i+1, \"/\", n_folds)\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size, input_shape=(32, 32, 3), padding='same', activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Conv2D(32, kernel_size, activation='relu', padding='same'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(dense_size, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        # Compile model\n",
    "        epochs = 75\n",
    "        decay = lrate/epochs\n",
    "        momentum = 0.9\n",
    "        sgd = SGD(lr=lrate, momentum=momentum, decay=decay)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "        fit_history = model.fit(X_tr_reshaped[train], y_train_one_hot[train], validation_data=(X_tr_reshaped[test], y_train_one_hot[test]), epochs=epochs, batch_size=32, verbose=2)\n",
    "\n",
    "        y_pred = model.predict(X_tr_reshaped[test], batch_size=32)\n",
    "        score = accuracy_score(y_train_one_hot[test].argmax(axis=1), y_pred.argmax(axis=1))\n",
    "\n",
    "        print (\"Score: \", score)\n",
    "        scores.append(score)\n",
    "    \n",
    "#         plt.plot(fit_history.history['acc'])\n",
    "        print\n",
    "        print ('acc:     ', [\"{0:0.3f}\".format(i) for i in fit_history.history['acc']])\n",
    "        print \n",
    "\n",
    "#         plt.plot(fit_history.history['val_acc'])\n",
    "        print ('val_acc: ', [\"{0:0.3f}\".format(i) for i in fit_history.history['val_acc']])\n",
    "        filename = 'acc_' + str(lrate) + '_' + str(i) + '.png'\n",
    "#         plt.savefig(filename)\n",
    "#         plt.close()\n",
    "\n",
    "    print (\"---> Score mean: \", np.mean(scores))\n",
    "    print \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki policzone na komputerze z GPU odostępnionym dzięki uprzejmości kolegi, Michała Goldy. Podsumowanie poniżej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>Dla danego lrate model osiąga wynik na poniższym poziomie po 75 epokach uczenia <br>\n",
    "lrate=0.0001 -> 57.8%\n",
    "lrate=0.0005 -> 64.1%\n",
    "lrate=0.001 -> 64.2% </pre>\n",
    "<i>Dokładne logi z przebiegów cross-validacji w plikach: 'out_0.0001.out', 'out_0.0005.out', 'out_0.001.out'</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zatem wytrenujmy model na całych danych dla <b>lrate=0.001</b> i zapiszmy wynik dla X_test, następnie wyślemy go na kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_reshaped = pack_color_image(X_test).reshape(-1, 32, 32, 3)\n",
    "\n",
    "dense_size = 1024\n",
    "dropout = 0.2\n",
    "kernel_size = (3,3)\n",
    "lrate = 0.001\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size, input_shape=(32, 32, 3), padding='same', activation='relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Conv2D(32, kernel_size, activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(dense_size, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "epochs = 100\n",
    "decay = lrate/epochs\n",
    "momentum = 0.9\n",
    "sgd = SGD(lr=lrate, momentum=momentum, decay=decay)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "fit_history = model.fit(X_tr_reshaped, y_train_one_hot, epochs=epochs, batch_size=32, verbose=2)\n",
    "\n",
    "y_pred = model.predict(X_test_reshaped, batch_size=32)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "\n",
    "save_labels(y_pred, 'y_pred_kowalik.csv')\n",
    "\n",
    "print (\"DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Log z przebiegu trenowania i predykcji w pliku: 'out_test_set.out'</i> \n",
    "\n",
    "Wynik: <br>\n",
    "<img src=\"./figures_mk/result_supervised.png\"> <br>\n",
    "<img src=\"./figures_mk/biceps.png\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Część II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Część druga polega na zaimplementowaniu autoenkodera. Wykorzystamy autoenkoder konwolucyjny na bazie powyższego modelu.\n",
    "\n",
    "Autoenkoderem jest sieć neuronowa stworzona z identycznych warstw jak powyżej, oraz dołożonych wartst o odwrotnym działaniu w odwrotnej kolejności. Taka sieć jest trenowana na danych (X_train, <b>X_train</b>), przez co uczy się dobrej reprezentacji danych, dla któych następuje kodowanie i dekodowanie. Jest to uczenie nienadzorowane, gdyż nie występują przy uczeniu etykiety danych, a autoenkoder stara się nauczyć takiej reprezentacji, które będą odzwierciedlały dane wejściowe. Następnie do części kodującej jest dokładana 'płaska' część sieci, identyczna jak w zadaniu powyżej, na której <b>dotrenowywany</b> jest wytrenowany już częściowo enkoder razem z płaską częścią sieci, dzięki czemu sieć pomimo nauczenia reprezentacji, jest w stanie nauczyć się odpowiednich etykiet.\n",
    "\n",
    "Założymy, że parametry są już dobrane przez model evaluation zasosowany powyżej, także parametrów dla sieci użyjemy tych, które okazały się najlepsze w pierwszej części."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 32, 32, 3)         867       \n",
      "=================================================================\n",
      "Total params: 29,507\n",
      "Trainable params: 29,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dense_size = 1024\n",
    "dropout = 0.2\n",
    "kernel_size = (3,3)\n",
    "lrate = 0.001\n",
    "\n",
    "input_img = Input(shape=(32, 32, 3))\n",
    "\n",
    "x = Conv2D(32, kernel_size, padding='same', activation='relu')(input_img)\n",
    "x = Dropout(dropout)(x)\n",
    "x = Conv2D(32, kernel_size, activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(32, kernel_size, activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, kernel_size, activation='relu', padding='same')(x)\n",
    "decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same', data_format=\"channels_last\")(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='categorical_crossentropy')\n",
    "\n",
    "encoder = Model(input_img, encoded)\n",
    "encoder.compile(optimizer='adadelta', loss='categorical_crossentropy')\n",
    "                       \n",
    "autoencoder.summary()\n",
    "\n",
    "x = Flatten()(encoded)\n",
    "x = Dense(dense_size, activation='relu', kernel_constraint=maxnorm(3))(x)\n",
    "x = Dropout(dropout)(x)\n",
    "output_layer = Dense(num_classes, activation='softmax')(x) \n",
    "\n",
    "model = Model(input_img, output_layer)\n",
    "epochs = 100\n",
    "decay = lrate/epochs\n",
    "momentum = 0.9\n",
    "sgd = SGD(lr=lrate, momentum=momentum, decay=decay)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(X_tr_reshaped,X_tr_reshaped,\n",
    "                epochs=epochs,\n",
    "                batch_size=32,\n",
    "                verbose=2)\n",
    "\n",
    "model.fit(X_train_small_reshaped, y_train_small_one_hot, epochs=epochs, batch_size=32, verbose=2)\n",
    "\n",
    "y_pred = model.predict(X_test_reshaped, batch_size=32)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "\n",
    "save_labels(y_pred, 'y_pred_unsupervised_kowalik.csv')\n",
    "\n",
    "print (\"DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>...\n",
    "Epoch 95/100\n",
    "3s - loss: 0.0090 - acc: 0.9986\n",
    "Epoch 96/100\n",
    "3s - loss: 0.0081 - acc: 0.9990\n",
    "Epoch 97/100\n",
    "3s - loss: 0.0066 - acc: 0.9992\n",
    "Epoch 98/100\n",
    "3s - loss: 0.0074 - acc: 0.9990\n",
    "Epoch 99/100\n",
    "3s - loss: 0.0060 - acc: 0.9996\n",
    "Epoch 100/100\n",
    "3s - loss: 0.0058 - acc: 0.9996\n",
    "DONE! </pre>\n",
    "<i>(pełny log w pliku 'out_test_set_unsup.out')</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wynik:\n",
    "<img src=\"./figures_mk/result_unsupervised.png\"><br>\n",
    "<img src=\"./figures_mk/biceps.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dla porównania wysłałem również predykcję, bez unsupervised-pretrainingu (autoenkodera):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# autoencoder.fit(X_tr_reshaped,X_tr_reshaped,\n",
    "#                 epochs=epochs,\n",
    "#                 batch_size=32,\n",
    "#                 verbose=2)\n",
    "\n",
    "model.fit(X_train_small_reshaped, y_train_small_one_hot, epochs=epochs, batch_size=32, verbose=2)\n",
    "\n",
    "y_pred = model.predict(X_test_reshaped, batch_size=32)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "\n",
    "save_labels(y_pred, 'y_pred_unsupervised_no_autoencoder_kowalik.csv')\n",
    "\n",
    "print (\"DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>...\n",
    "Epoch 96/100\n",
    "3s - loss: 0.0149 - acc: 0.9982\n",
    "Epoch 97/100\n",
    "3s - loss: 0.0140 - acc: 0.9978\n",
    "Epoch 98/100\n",
    "3s - loss: 0.0177 - acc: 0.9968\n",
    "Epoch 99/100\n",
    "3s - loss: 0.0198 - acc: 0.9958\n",
    "Epoch 100/100\n",
    "3s - loss: 0.0155 - acc: 0.9976\n",
    "DONE!\n",
    "</pre>\n",
    "<i>Pełny log w pliku 'out_test_set_unsup_no_autoencoder.out'</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wynik bez pretrainingu był o ponad 2,5% (punktu procentowego) gorszy. <br>\n",
    "<img src=\"./figures_mk/result_unsupervised_no_autoencoder.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
